{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floating-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def merge_word_analogy_experiments(\n",
    "    directory_path: str = \"../Outer_Correlation/new_outer_correlation_results_per_section/\",\n",
    "    output_filename: str = \"merged_word_analogy_results.csv\",\n",
    "    output_dir: str = \".\",\n",
    "    validate_data: bool = True,\n",
    "    create_summary: bool = True\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Comprehensive merging of word analogy experiment results across all 14 categories.\n",
    "    \n",
    "    Designed specifically for word analogy research comparing similarity measures\n",
    "    across semantic and syntactic relationship types.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing the 14 CSV files\n",
    "        output_filename: Name for the merged CSV file\n",
    "        output_dir: Directory to save the merged file\n",
    "        validate_data: Whether to perform data validation checks\n",
    "        create_summary: Whether to create detailed summary statistics\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (merged_dataframe, summary_statistics)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting comprehensive merge of word analogy experiments...\")\n",
    "    print(f\"Looking for CSV files in: {directory_path}\")\n",
    "    \n",
    "    # Get all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(directory_path, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {directory_path}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to merge\")\n",
    "    \n",
    "    # Expected categories\n",
    "    expected_categories = {\n",
    "        'capital_common_countries_results.csv': 'capital-common-countries',\n",
    "        'capital_world_results.csv': 'capital-world',\n",
    "        'currency_results.csv': 'currency',\n",
    "        'city_in_state_results.csv': 'city-in-state',\n",
    "        'family_results.csv': 'family',\n",
    "        'gram1_adjective_to_adverb_results.csv': 'gram1-adjective-to-adverb',\n",
    "        'gram2_opposite_results.csv': 'gram2-opposite',\n",
    "        'gram3_comparative_results.csv': 'gram3-comparative',\n",
    "        'gram4_superlative_results.csv': 'gram4-superlative',\n",
    "        'gram5_present_participle_results.csv': 'gram5-present-participle',\n",
    "        'gram6_nationality_adjective_results.csv': 'gram6-nationality-adjective',\n",
    "        'gram7_past_tense_results.csv': 'gram7-past-tense',\n",
    "        'gram8_plural_results.csv': 'gram8-plural',\n",
    "        'gram9_plural_verbs_results.csv': 'gram9-plural-verbs'\n",
    "    }\n",
    "    \n",
    "    # Define semantic vs syntactic categories for analysis\n",
    "    semantic_categories = {\n",
    "        'capital-common-countries', 'capital-world', 'currency', \n",
    "        'city-in-state', 'family'\n",
    "    }\n",
    "    \n",
    "    dataframes = []\n",
    "    dataset_metadata = {}\n",
    "    validation_issues = []\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for i, file_path in enumerate(sorted(csv_files)):\n",
    "        filename = os.path.basename(file_path)\n",
    "        dataset_name = filename.replace('.csv', '')\n",
    "        \n",
    "        print(f\" Processing {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            original_rows = len(df)\n",
    "            \n",
    "            # Add source tracking metadata\n",
    "            df['dataset_source'] = dataset_name\n",
    "            df['dataset_id'] = i\n",
    "            df['file_path'] = file_path\n",
    "            df['filename'] = filename\n",
    "            \n",
    "            # Validate expected category if we can map it\n",
    "            expected_category = expected_categories.get(filename)\n",
    "            if expected_category and 'category' in df.columns:\n",
    "                actual_categories = df['category'].unique()\n",
    "                if len(actual_categories) == 1 and actual_categories[0] != expected_category:\n",
    "                    validation_issues.append(\n",
    "                        f\"Category mismatch in {filename}: expected {expected_category}, \"\n",
    "                        f\"found {actual_categories[0]}\"\n",
    "                    )\n",
    "            \n",
    "            # Ensure category_type is properly set\n",
    "            if 'category' in df.columns and 'category_type' in df.columns:\n",
    "                df['category_type'] = df['category'].apply(\n",
    "                    lambda x: 'semantic' if x in semantic_categories else 'syntactic'\n",
    "                )\n",
    "            \n",
    "            # Store metadata about this dataset\n",
    "            dataset_metadata[dataset_name] = {\n",
    "                'filename': filename,\n",
    "                'rows': original_rows,\n",
    "                'categories': df['category'].unique().tolist() if 'category' in df.columns else [],\n",
    "                'measures': df['measure'].unique().tolist() if 'measure' in df.columns else [],\n",
    "                'quantiles': sorted(df['quantile'].unique().tolist()) if 'quantile' in df.columns else [],\n",
    "                'rcond_values': sorted(df['rcond'].unique().tolist()) if 'rcond' in df.columns else [],\n",
    "                'top_k_values': sorted(df['top@k'].unique().tolist()) if 'top@k' in df.columns else [],\n",
    "                'accuracy_range': (\n",
    "                    df['overall_accuracy'].min(), \n",
    "                    df['overall_accuracy'].max()\n",
    "                ) if 'overall_accuracy' in df.columns else (None, None),\n",
    "                'mean_accuracy': df['overall_accuracy'].mean() if 'overall_accuracy' in df.columns else None\n",
    "            }\n",
    "            \n",
    "            dataframes.append(df)\n",
    "            print(f\"   Loaded {original_rows} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {filename}: {str(e)}\"\n",
    "            print(f\"   {error_msg}\")\n",
    "            validation_issues.append(error_msg)\n",
    "            continue\n",
    "    \n",
    "    if not dataframes:\n",
    "        raise RuntimeError(\"No valid dataframes were loaded!\")\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    print(\"\\n Merging all datasets...\")\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    original_total_rows = len(merged_df)\n",
    "    \n",
    "    # Add computed features for analysis\n",
    "    print(\" Computing analysis features...\")\n",
    "    \n",
    "    # Create parameter combination identifier\n",
    "    if all(col in merged_df.columns for col in ['quantile', 'rcond', 'measure']):\n",
    "        merged_df['param_combination'] = (\n",
    "            merged_df['quantile'].astype(str) + '_' + \n",
    "            merged_df['rcond'].astype(str) + '_' + \n",
    "            merged_df['measure'].astype(str)\n",
    "        )\n",
    "    \n",
    "    # Add percentile rankings within each dataset\n",
    "    if 'overall_accuracy' in merged_df.columns and 'dataset_source' in merged_df.columns:\n",
    "        merged_df['accuracy_percentile_within_dataset'] = merged_df.groupby('dataset_source')['overall_accuracy'].rank(pct=True)\n",
    "        merged_df['accuracy_percentile_global'] = merged_df['overall_accuracy'].rank(pct=True)\n",
    "        merged_df['is_top_10_percent_within_dataset'] = merged_df['accuracy_percentile_within_dataset'] >= 0.9\n",
    "        merged_df['is_top_10_percent_global'] = merged_df['accuracy_percentile_global'] >= 0.9\n",
    "    \n",
    "    # Add category-level rankings\n",
    "    if all(col in merged_df.columns for col in ['category', 'overall_accuracy']):\n",
    "        merged_df['accuracy_rank_within_category'] = merged_df.groupby('category')['overall_accuracy'].rank(ascending=False)\n",
    "        merged_df['is_best_in_category'] = merged_df['accuracy_rank_within_category'] == 1\n",
    "    \n",
    "    # Add measure performance indicators\n",
    "    if all(col in merged_df.columns for col in ['measure', 'overall_accuracy']):\n",
    "        merged_df['accuracy_rank_within_measure'] = merged_df.groupby('measure')['overall_accuracy'].rank(ascending=False)\n",
    "    \n",
    "    # Validation checks\n",
    "    if validate_data:\n",
    "        print(\"\\n Performing data validation...\")\n",
    "        \n",
    "        # Check for expected columns\n",
    "        expected_columns = [\n",
    "            'word1', 'word2', 'word3', 'true_word', 'category', 'category_type',\n",
    "            'quantile', 'rcond', 'measure', 'top@k', 'overall_accuracy'\n",
    "        ]\n",
    "        missing_columns = [col for col in expected_columns if col not in merged_df.columns]\n",
    "        if missing_columns:\n",
    "            validation_issues.append(f\"Missing expected columns: {missing_columns}\")\n",
    "        \n",
    "        # Check for duplicate rows\n",
    "        duplicate_rows = merged_df.duplicated().sum()\n",
    "        if duplicate_rows > 0:\n",
    "            validation_issues.append(f\"Found {duplicate_rows} duplicate rows\")\n",
    "        \n",
    "        # Check for missing values in key columns\n",
    "        key_columns = ['category', 'measure', 'overall_accuracy']\n",
    "        for col in key_columns:\n",
    "            if col in merged_df.columns:\n",
    "                missing_count = merged_df[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    validation_issues.append(f\"Missing values in {col}: {missing_count}\")\n",
    "        \n",
    "        # Print validation results\n",
    "        if validation_issues:\n",
    "            print(\"  Validation Issues Found:\")\n",
    "            for issue in validation_issues:\n",
    "                print(f\"   - {issue}\")\n",
    "        else:\n",
    "            print(\" All validation checks passed!\")\n",
    "    \n",
    "    # Create comprehensive summary statistics\n",
    "    summary_stats = {\n",
    "        'merge_info': {\n",
    "            'total_files_processed': len(csv_files),\n",
    "            'successful_files': len(dataframes),\n",
    "            'total_rows': len(merged_df),\n",
    "            'unique_categories': merged_df['category'].nunique() if 'category' in merged_df.columns else 0,\n",
    "            'unique_measures': merged_df['measure'].nunique() if 'measure' in merged_df.columns else 0,\n",
    "            'unique_param_combinations': merged_df['param_combination'].nunique() if 'param_combination' in merged_df.columns else 0,\n",
    "        },\n",
    "        'dataset_metadata': dataset_metadata,\n",
    "        'validation_issues': validation_issues,\n",
    "    }\n",
    "    \n",
    "    if create_summary and 'overall_accuracy' in merged_df.columns:\n",
    "        print(\" Creating detailed summary statistics...\")\n",
    "        \n",
    "        # Overall performance summary\n",
    "        summary_stats['performance_summary'] = {\n",
    "            'overall_accuracy_range': (merged_df['overall_accuracy'].min(), merged_df['overall_accuracy'].max()),\n",
    "            'overall_accuracy_mean': merged_df['overall_accuracy'].mean(),\n",
    "            'overall_accuracy_std': merged_df['overall_accuracy'].std(),\n",
    "        }\n",
    "        \n",
    "        # Performance by category\n",
    "        if 'category' in merged_df.columns:\n",
    "            category_stats = merged_df.groupby('category')['overall_accuracy'].agg([\n",
    "                'count', 'mean', 'std', 'min', 'max'\n",
    "            ]).round(4)\n",
    "            summary_stats['category_performance'] = category_stats.to_dict('index')\n",
    "        \n",
    "        # Performance by measure\n",
    "        if 'measure' in merged_df.columns:\n",
    "            measure_stats = merged_df.groupby('measure')['overall_accuracy'].agg([\n",
    "                'count', 'mean', 'std', 'min', 'max'\n",
    "            ]).round(4)\n",
    "            summary_stats['measure_performance'] = measure_stats.to_dict('index')\n",
    "        \n",
    "        # Performance by category type\n",
    "        if 'category_type' in merged_df.columns:\n",
    "            category_type_stats = merged_df.groupby('category_type')['overall_accuracy'].agg([\n",
    "                'count', 'mean', 'std', 'min', 'max'\n",
    "            ]).round(4)\n",
    "            summary_stats['category_type_performance'] = category_type_stats.to_dict('index')\n",
    "        \n",
    "        # Best parameter combinations\n",
    "        if 'param_combination' in merged_df.columns:\n",
    "            best_params = merged_df.nlargest(20, 'overall_accuracy')[\n",
    "                ['param_combination', 'category', 'measure', 'quantile', 'rcond', 'overall_accuracy']\n",
    "            ]\n",
    "            summary_stats['top_parameter_combinations'] = best_params.to_dict('records')\n",
    "    \n",
    "    # Save the merged dataset\n",
    "    output_path = Path(output_dir) / output_filename\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n Saving merged dataset to: {output_path}\")\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Also save summary statistics as JSON\n",
    "    summary_path = Path(output_dir) / f\"summary_{output_filename.replace('.csv', '.json')}\"\n",
    "    import json\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    def convert_numpy_types(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "    \n",
    "    # Clean summary stats for JSON serialization\n",
    "    json_summary = {}\n",
    "    for key, value in summary_stats.items():\n",
    "        if isinstance(value, dict):\n",
    "            json_summary[key] = {k: convert_numpy_types(v) for k, v in value.items()}\n",
    "        else:\n",
    "            json_summary[key] = convert_numpy_types(value)\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(json_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\" Summary statistics saved to: {summary_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n Merge completed successfully!\")\n",
    "    print(f\" Final dataset: {len(merged_df):,} rows, {len(merged_df.columns)} columns\")\n",
    "    print(f\"  Categories: {merged_df['category'].nunique() if 'category' in merged_df.columns else 'N/A'}\")\n",
    "    print(f\" Measures: {merged_df['measure'].nunique() if 'measure' in merged_df.columns else 'N/A'}\")\n",
    "    print(f\" Accuracy range: {merged_df['overall_accuracy'].min():.4f} - {merged_df['overall_accuracy'].max():.4f}\")\n",
    "    \n",
    "    return merged_df, summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "understanding-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick usage function\n",
    "def quick_merge_data():\n",
    "    \"\"\"Quick function to merge word analogy data with sensible defaults\"\"\"\n",
    "    return merge_word_analogy_experiments(\n",
    "        directory_path=\"../Outer_Correlation/new_outer_correlation_results_per_section/\",\n",
    "        output_filename=\"oc_results.csv\",\n",
    "        output_dir=\"../Outer_Correlation/final_results/\",\n",
    "        validate_data=True,\n",
    "        create_summary=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "molecular-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive merge of word analogy experiments...\n",
      "Looking for CSV files in: ../Outer_Correlation/new_outer_correlation_results_per_section/\n",
      "Found 14 CSV files to merge\n",
      " Processing capital_common_countries_results.csv...\n",
      "   Loaded 30360 rows\n",
      " Processing capital_world_results.csv...\n",
      "   Loaded 71556 rows\n",
      " Processing city_in_state_results.csv...\n",
      "   Loaded 112488 rows\n",
      " Processing currency_results.csv...\n",
      "   Loaded 7680 rows\n",
      " Processing family_results.csv...\n",
      "   Loaded 20520 rows\n",
      " Processing gram1_adjective_to_adverb_results.csv...\n",
      "   Loaded 48720 rows\n",
      " Processing gram2_opposite_results.csv...\n",
      "   Loaded 22800 rows\n",
      " Processing gram3_comparative_results.csv...\n",
      "   Loaded 79920 rows\n",
      " Processing gram4_superlative_results.csv...\n",
      "   Loaded 27144 rows\n",
      " Processing gram5_present_participle_results.csv...\n",
      "   Loaded 52200 rows\n",
      " Processing gram6_nationality_adjective_results.csv...\n",
      "   Loaded 73740 rows\n",
      " Processing gram7_past_tense_results.csv...\n",
      "   Loaded 88920 rows\n",
      " Processing gram8_plural_results.csv...\n",
      "   Loaded 59520 rows\n",
      " Processing gram9_plural_verbs_results.csv...\n",
      "   Loaded 42120 rows\n",
      "\n",
      " Merging all datasets...\n",
      " Computing analysis features...\n",
      "\n",
      " Performing data validation...\n",
      " All validation checks passed!\n",
      " Creating detailed summary statistics...\n",
      "\n",
      " Saving merged dataset to: ../Outer_Correlation/final_results/oc_results.csv\n",
      " Summary statistics saved to: ../Outer_Correlation/final_results/summary_oc_results.json\n",
      "\n",
      " Merge completed successfully!\n",
      " Final dataset: 737,688 rows, 34 columns\n",
      "  Categories: 14\n",
      " Measures: 3\n",
      " Accuracy range: 0.4249 - 1.0000\n"
     ]
    }
   ],
   "source": [
    "merged_data, summary = quick_merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "competent-tooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset overview:\n",
      "Shape: (737688, 34)\n",
      "Categories: ['capital-common-countries', 'capital-world', 'city-in-state', 'currency', 'family', 'gram1-adjective-to-adverb', 'gram2-opposite', 'gram3-comparative', 'gram4-superlative', 'gram5-present-participle', 'gram6-nationality-adjective', 'gram7-past-tense', 'gram8-plural', 'gram9-plural-verbs']\n",
      "Measures: ['mahalanobis_cosine', 'mahalanobis_shifted_cosine', 'naive_cosine']\n",
      "Quantiles tested: [0.01, 0.05, 0.1, 0.25, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset overview:\")\n",
    "print(f\"Shape: {merged_data.shape}\")\n",
    "print(f\"Categories: {sorted(merged_data['category'].unique())}\")\n",
    "print(f\"Measures: {sorted(merged_data['measure'].unique())}\")\n",
    "print(f\"Quantiles tested: {sorted(merged_data['quantile'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-junction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
