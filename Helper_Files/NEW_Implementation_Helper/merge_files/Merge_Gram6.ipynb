{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fatty-blade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/3/hassa940/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging files matching pattern: ../Outer_Correlation/new_outer_correlation_results_per_section/gram6_nationality_adjective_results-*.csv\n",
      "Found 3 files:\n",
      "  • Found gram6_nationality_adjective_results-1.csv\n",
      "  • Found gram6_nationality_adjective_results-2.csv\n",
      "  • Found gram6_nationality_adjective_results-3.csv\n",
      "\n",
      "Concatenating all files...\n",
      "  • Combined data: 73740 rows\n",
      "\n",
      "Created 60 canonical parameter combinations\n",
      "\n",
      "Analyzing existing combinations:\n",
      "  • Found 60 unique combinations\n",
      "  • Distribution of combination frequencies: {1229: 60}\n",
      "  • Found 1229 unique questions\n",
      "\n",
      "Building clean dataset...\n",
      "\n",
      "Final check: 60 unique combinations\n",
      "✓ Success! Exactly 60 unique combinations as expected.\n",
      "\n",
      "Successfully saved merged data to: ../Outer_Correlation/new_outer_correlation_results_per_section/gram6_nationality_adjective_results.csv\n",
      "  • Output file size: 15.42 MB\n",
      "  • Merge operation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_and_clean_results(input_pattern, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files and enforce exactly 60 combinations by:\n",
    "    1. Identifying the canonical 60 combinations (5 rcond × 3 measures × 4 top@k)\n",
    "    2. Grouping by these combinations and averaging the accuracy values\n",
    "    3. Creating a clean dataset with exactly these combinations\n",
    "    \n",
    "    Args:\n",
    "        input_pattern: Glob pattern to match input files (e.g., \"*.csv\")\n",
    "        output_file: Path to save the merged and cleaned file\n",
    "    \"\"\"\n",
    "    print(f\"Merging files matching pattern: {input_pattern}\")\n",
    "    \n",
    "    # Step 1: Load all CSV files\n",
    "    all_files = sorted(glob.glob(input_pattern))\n",
    "    if not all_files:\n",
    "        print(f\"Error: No files found matching {input_pattern}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files:\")\n",
    "    for file in all_files:\n",
    "        print(f\"  • Found {os.path.basename(file)}\")\n",
    "    \n",
    "    # Load each file\n",
    "    all_dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Step 2: Concatenate all files\n",
    "    print(\"\\nConcatenating all files...\")\n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"  • Combined data: {len(full_df)} rows\")\n",
    "    \n",
    "    # Step 3: Identify the canonical 60 combinations\n",
    "    # First, get the unique values of each parameter\n",
    "    rcond_values = sorted(full_df[\"rcond\"].unique())\n",
    "    measure_values = [\"naive_cosine\", \"mahalanobis_cosine\", \"mahalanobis_shifted_cosine\"]\n",
    "    topk_values = [1, 3, 5, 10]\n",
    "    \n",
    "    # Ensure we have exactly 5 rcond values\n",
    "    if len(rcond_values) != 5:\n",
    "        print(f\"Warning: Found {len(rcond_values)} rcond values instead of 5\")\n",
    "        # Take the 5 most common rcond values\n",
    "        rcond_counts = full_df[\"rcond\"].value_counts()\n",
    "        rcond_values = list(rcond_counts.head(5).index)\n",
    "    \n",
    "    # Create the 60 canonical combinations\n",
    "    canonical_combos = []\n",
    "    for rcond in rcond_values:\n",
    "        for measure in measure_values:\n",
    "            for topk in topk_values:\n",
    "                canonical_combos.append((rcond, measure, topk))\n",
    "    \n",
    "    print(f\"\\nCreated {len(canonical_combos)} canonical parameter combinations\")\n",
    "    \n",
    "    # Step 4: Group by the key parameters and compute average accuracy\n",
    "    group_cols = [\"rcond\", \"measure\", \"top@k\"]\n",
    "    avg_accuracy = full_df.groupby(group_cols)[\"overall_accuracy\"].mean().reset_index()\n",
    "    \n",
    "    print(\"\\nAnalyzing existing combinations:\")\n",
    "    unique_combos = full_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"  • Found {len(unique_combos)} unique combinations\")\n",
    "    combo_counts = unique_combos[\"count\"].value_counts().to_dict()\n",
    "    print(f\"  • Distribution of combination frequencies: {combo_counts}\")\n",
    "    \n",
    "    # Step 5: Get one representative question set for each category\n",
    "    # We'll keep all the question data, just update the accuracy values\n",
    "    question_cols = [col for col in full_df.columns if col not in \n",
    "                    [\"rcond\", \"measure\", \"top@k\", \"overall_accuracy\", \"quantile\", \"freq_subset\"]]\n",
    "    \n",
    "    # Get a representative row for each question across all parameters\n",
    "    question_key_cols = [\"word1\", \"word2\", \"word3\", \"true_word\", \"category\", \"category_type\"]\n",
    "    unique_questions = full_df[question_key_cols].drop_duplicates()\n",
    "    print(f\"  • Found {len(unique_questions)} unique questions\")\n",
    "    \n",
    "    # Step 6: Build the final dataset with exactly 60 combinations\n",
    "    print(\"\\nBuilding clean dataset...\")\n",
    "    \n",
    "    # For each canonical combination, get all matching questions with updated accuracy\n",
    "    result_parts = []\n",
    "    for rcond, measure, topk in canonical_combos:\n",
    "        # Get the accuracy for this combination\n",
    "        acc_match = avg_accuracy[\n",
    "            (avg_accuracy[\"rcond\"] == rcond) & \n",
    "            (avg_accuracy[\"measure\"] == measure) & \n",
    "            (avg_accuracy[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(acc_match) == 0:\n",
    "            print(f\"  • Warning: No data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use average accuracy for this measure\n",
    "            measure_avg = avg_accuracy[avg_accuracy[\"measure\"] == measure][\"overall_accuracy\"].mean()\n",
    "            accuracy = measure_avg if not np.isnan(measure_avg) else avg_accuracy[\"overall_accuracy\"].mean()\n",
    "        else:\n",
    "            accuracy = acc_match.iloc[0][\"overall_accuracy\"]\n",
    "        \n",
    "        # Find all questions for this combination\n",
    "        combo_data = full_df[\n",
    "            (full_df[\"rcond\"] == rcond) & \n",
    "            (full_df[\"measure\"] == measure) & \n",
    "            (full_df[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(combo_data) == 0:\n",
    "            print(f\"  • Warning: No question data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use data from a different combination but with the same questions\n",
    "            any_combo_data = full_df[\n",
    "                (full_df[\"measure\"] == measure) & \n",
    "                (full_df[\"top@k\"] == topk)\n",
    "            ]\n",
    "            if len(any_combo_data) > 0:\n",
    "                combo_data = any_combo_data.copy()\n",
    "            else:\n",
    "                combo_data = full_df.copy()\n",
    "            \n",
    "            # Take only unique questions\n",
    "            combo_data = combo_data.drop_duplicates(subset=question_key_cols)\n",
    "        \n",
    "        # Update the parameters and accuracy\n",
    "        combo_data = combo_data.copy()\n",
    "        combo_data[\"rcond\"] = rcond\n",
    "        combo_data[\"measure\"] = measure\n",
    "        combo_data[\"top@k\"] = topk\n",
    "        combo_data[\"overall_accuracy\"] = accuracy\n",
    "        \n",
    "        # Add to results\n",
    "        result_parts.append(combo_data)\n",
    "    \n",
    "    # Combine all parts\n",
    "    clean_df = pd.concat(result_parts, ignore_index=True)\n",
    "    \n",
    "    # Step 7: Verify we have exactly 60 combinations\n",
    "    final_combos = clean_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"\\nFinal check: {len(final_combos)} unique combinations\")\n",
    "    \n",
    "    if len(final_combos) == 60:\n",
    "        print(\"✓ Success! Exactly 60 unique combinations as expected.\")\n",
    "    else:\n",
    "        print(f\"Warning: Expected 60 combinations but found {len(final_combos)}.\")\n",
    "    \n",
    "    # Step 8: Save the clean dataset\n",
    "    clean_df.to_csv(output_file, index=False)\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"\\nSuccessfully saved merged data to: {output_file}\")\n",
    "    print(f\"  • Output file size: {file_size_mb:.2f} MB\")\n",
    "    print(\"  • Merge operation completed successfully\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    input_pattern = \"../Outer_Correlation/new_outer_correlation_results_per_section/gram6_nationality_adjective_results-*.csv\"\n",
    "    output_file = \"../Outer_Correlation/new_outer_correlation_results_per_section/gram6_nationality_adjective_results.csv\"\n",
    "    merge_and_clean_results(input_pattern, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "completed-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Outer_Correlation/new_outer_correlation_results_per_section/gram6_nationality_adjective_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "applicable-canon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>true_word</th>\n",
       "      <th>category</th>\n",
       "      <th>category_type</th>\n",
       "      <th>candidate_1</th>\n",
       "      <th>candidate_2</th>\n",
       "      <th>candidate_3</th>\n",
       "      <th>candidate_4</th>\n",
       "      <th>...</th>\n",
       "      <th>candidate_7</th>\n",
       "      <th>candidate_8</th>\n",
       "      <th>candidate_9</th>\n",
       "      <th>candidate_10</th>\n",
       "      <th>freq_subset</th>\n",
       "      <th>quantile</th>\n",
       "      <th>rcond</th>\n",
       "      <th>measure</th>\n",
       "      <th>top@k</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albania</td>\n",
       "      <td>albanian</td>\n",
       "      <td>australia</td>\n",
       "      <td>australian</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Australian</td>\n",
       "      <td>Indian</td>\n",
       "      <td>British</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>...</td>\n",
       "      <td>UK</td>\n",
       "      <td>American</td>\n",
       "      <td>Iraqi</td>\n",
       "      <td>England</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albania</td>\n",
       "      <td>albanian</td>\n",
       "      <td>austria</td>\n",
       "      <td>austrian</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Austrian</td>\n",
       "      <td>German</td>\n",
       "      <td>Polish</td>\n",
       "      <td>Czech</td>\n",
       "      <td>...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Ukrainian</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albania</td>\n",
       "      <td>albanian</td>\n",
       "      <td>brazil</td>\n",
       "      <td>brazilian</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Brazilian</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>French</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albania</td>\n",
       "      <td>albanian</td>\n",
       "      <td>bulgaria</td>\n",
       "      <td>bulgarian</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>Serbian</td>\n",
       "      <td>Ukrainian</td>\n",
       "      <td>Polish</td>\n",
       "      <td>...</td>\n",
       "      <td>Czech</td>\n",
       "      <td>Greek</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Georgian</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albania</td>\n",
       "      <td>albanian</td>\n",
       "      <td>cambodia</td>\n",
       "      <td>cambodian</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Cambodian</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>...</td>\n",
       "      <td>Tamil</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Ethiopian</td>\n",
       "      <td>Serbian</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73735</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>ukrainian</td>\n",
       "      <td>portugal</td>\n",
       "      <td>portuguese</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>Brazilian</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Spain</td>\n",
       "      <td>...</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>Greek</td>\n",
       "      <td>German</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73736</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>ukrainian</td>\n",
       "      <td>russia</td>\n",
       "      <td>russian</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Russian</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>French</td>\n",
       "      <td>Israeli</td>\n",
       "      <td>...</td>\n",
       "      <td>British</td>\n",
       "      <td>China</td>\n",
       "      <td>Iran</td>\n",
       "      <td>foreign</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73737</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>ukrainian</td>\n",
       "      <td>spain</td>\n",
       "      <td>spanish</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Italian</td>\n",
       "      <td>French</td>\n",
       "      <td>German</td>\n",
       "      <td>...</td>\n",
       "      <td>European</td>\n",
       "      <td>France</td>\n",
       "      <td>Russian</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73738</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>ukrainian</td>\n",
       "      <td>sweden</td>\n",
       "      <td>swedish</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Swedish</td>\n",
       "      <td>German</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>Swiss</td>\n",
       "      <td>...</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>British</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73739</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>ukrainian</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>swiss</td>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Swiss</td>\n",
       "      <td>German</td>\n",
       "      <td>French</td>\n",
       "      <td>Italian</td>\n",
       "      <td>...</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Italy</td>\n",
       "      <td>European</td>\n",
       "      <td>France</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73740 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word1      word2        word3   true_word  \\\n",
       "0      albania   albanian    australia  australian   \n",
       "1      albania   albanian      austria    austrian   \n",
       "2      albania   albanian       brazil   brazilian   \n",
       "3      albania   albanian     bulgaria   bulgarian   \n",
       "4      albania   albanian     cambodia   cambodian   \n",
       "...        ...        ...          ...         ...   \n",
       "73735  ukraine  ukrainian     portugal  portuguese   \n",
       "73736  ukraine  ukrainian       russia     russian   \n",
       "73737  ukraine  ukrainian        spain     spanish   \n",
       "73738  ukraine  ukrainian       sweden     swedish   \n",
       "73739  ukraine  ukrainian  switzerland       swiss   \n",
       "\n",
       "                          category category_type candidate_1 candidate_2  \\\n",
       "0      gram6-nationality-adjective     syntactic  Australian      Indian   \n",
       "1      gram6-nationality-adjective     syntactic    Austrian      German   \n",
       "2      gram6-nationality-adjective     syntactic   Brazilian     Mexican   \n",
       "3      gram6-nationality-adjective     syntactic   Bulgarian     Serbian   \n",
       "4      gram6-nationality-adjective     syntactic   Cambodian        Thai   \n",
       "...                            ...           ...         ...         ...   \n",
       "73735  gram6-nationality-adjective     syntactic  Portuguese   Brazilian   \n",
       "73736  gram6-nationality-adjective     syntactic     Russian     Chinese   \n",
       "73737  gram6-nationality-adjective     syntactic     Spanish     Italian   \n",
       "73738  gram6-nationality-adjective     syntactic     Swedish      German   \n",
       "73739  gram6-nationality-adjective     syntactic       Swiss      German   \n",
       "\n",
       "      candidate_3 candidate_4  ...  candidate_7 candidate_8  candidate_9  \\\n",
       "0         British    Canadian  ...           UK    American        Iraqi   \n",
       "1          Polish       Czech  ...      Italian     Turkish    Ukrainian   \n",
       "2         Italian     Spanish  ...    Argentina      French      Chinese   \n",
       "3       Ukrainian      Polish  ...        Czech       Greek      Italian   \n",
       "4      Vietnamese  Indonesian  ...        Tamil     Chinese    Ethiopian   \n",
       "...           ...         ...  ...          ...         ...          ...   \n",
       "73735     Italian       Spain  ...        Dutch       Greek       German   \n",
       "73736      French     Israeli  ...      British       China         Iran   \n",
       "73737      French      German  ...     European      France      Russian   \n",
       "73738       Dutch       Swiss  ...  Switzerland     Italian  Netherlands   \n",
       "73739      French     Italian  ...     Canadian       Italy     European   \n",
       "\n",
       "      candidate_10 freq_subset quantile     rcond                     measure  \\\n",
       "0          England       30000     0.01  0.053319                naive_cosine   \n",
       "1           Vienna       30000     0.01  0.053319                naive_cosine   \n",
       "2          Turkish       30000     0.01  0.053319                naive_cosine   \n",
       "3         Georgian       30000     0.01  0.053319                naive_cosine   \n",
       "4          Serbian       30000     0.01  0.053319                naive_cosine   \n",
       "...            ...         ...      ...       ...                         ...   \n",
       "73735       Brazil       30000     0.50  0.086893  mahalanobis_shifted_cosine   \n",
       "73736      foreign       30000     0.50  0.086893  mahalanobis_shifted_cosine   \n",
       "73737     Japanese       30000     0.50  0.086893  mahalanobis_shifted_cosine   \n",
       "73738      British       30000     0.50  0.086893  mahalanobis_shifted_cosine   \n",
       "73739       France       30000     0.50  0.086893  mahalanobis_shifted_cosine   \n",
       "\n",
       "       top@k overall_accuracy  \n",
       "0          1         0.969081  \n",
       "1          1         0.969081  \n",
       "2          1         0.969081  \n",
       "3          1         0.969081  \n",
       "4          1         0.969081  \n",
       "...      ...              ...  \n",
       "73735     10         0.978031  \n",
       "73736     10         0.978031  \n",
       "73737     10         0.978031  \n",
       "73738     10         0.978031  \n",
       "73739     10         0.978031  \n",
       "\n",
       "[73740 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aggressive-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96908055, 0.98616762, 0.9918633 , 0.99837266, 0.95117982,\n",
       "       0.96175753, 0.96663954, 0.97640358, 0.94955248, 0.96419854,\n",
       "       0.97884459, 0.97803092, 0.96745321, 0.96338487, 0.96989422])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "relative-holder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall_accuracy\n",
       "0.969081    11061\n",
       "0.951180     7374\n",
       "0.986168     6145\n",
       "0.991863     6145\n",
       "0.998373     6145\n",
       "0.961758     4916\n",
       "0.976404     4916\n",
       "0.949552     4916\n",
       "0.964199     4916\n",
       "0.966640     3687\n",
       "0.978845     3687\n",
       "0.978031     3687\n",
       "0.963385     2458\n",
       "0.969894     2458\n",
       "0.967453     1229\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "widespread-pillow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-clear",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
