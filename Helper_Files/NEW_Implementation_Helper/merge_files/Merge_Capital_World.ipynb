{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "changed-address",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/3/hassa940/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging files matching pattern: ../Outer_Correlation/new_outer_correlation_results_per_section/capital_world_results-*.csv\n",
      "Found 8 files:\n",
      "  • Found capital_world_results-1.csv\n",
      "  • Found capital_world_results-2.csv\n",
      "  • Found capital_world_results-3.csv\n",
      "  • Found capital_world_results-4.csv\n",
      "  • Found capital_world_results-5.csv\n",
      "  • Found capital_world_results-6.csv\n",
      "  • Found capital_world_results-7.csv\n",
      "  • Found capital_world_results-8.csv\n",
      "\n",
      "Concatenating all files...\n",
      "  • Combined data: 82980 rows\n",
      "Warning: Found 9 rcond values instead of 5\n",
      "\n",
      "Created 60 canonical parameter combinations\n",
      "\n",
      "Analyzing existing combinations:\n",
      "  • Found 108 unique combinations\n",
      "  • Distribution of combination frequencies: {1145: 48, 238: 48, 1383: 12}\n",
      "  • Found 1383 unique questions\n",
      "\n",
      "Building clean dataset...\n",
      "\n",
      "Final check: 60 unique combinations\n",
      "✓ Success! Exactly 60 unique combinations as expected.\n",
      "\n",
      "Successfully saved merged data to: ../Outer_Correlation/new_outer_correlation_results_per_section/capital_world_results.csv\n",
      "  • Output file size: 13.86 MB\n",
      "  • Merge operation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_and_clean_results(input_pattern, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files and enforce exactly 60 combinations by:\n",
    "    1. Identifying the canonical 60 combinations (5 rcond × 3 measures × 4 top@k)\n",
    "    2. Grouping by these combinations and averaging the accuracy values\n",
    "    3. Creating a clean dataset with exactly these combinations\n",
    "    \n",
    "    Args:\n",
    "        input_pattern: Glob pattern to match input files (e.g., \"*.csv\")\n",
    "        output_file: Path to save the merged and cleaned file\n",
    "    \"\"\"\n",
    "    print(f\"Merging files matching pattern: {input_pattern}\")\n",
    "    \n",
    "    # Step 1: Load all CSV files\n",
    "    all_files = sorted(glob.glob(input_pattern))\n",
    "    if not all_files:\n",
    "        print(f\"Error: No files found matching {input_pattern}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files:\")\n",
    "    for file in all_files:\n",
    "        print(f\"  • Found {os.path.basename(file)}\")\n",
    "    \n",
    "    # Load each file\n",
    "    all_dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Step 2: Concatenate all files\n",
    "    print(\"\\nConcatenating all files...\")\n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"  • Combined data: {len(full_df)} rows\")\n",
    "    \n",
    "    # Step 3: Identify the canonical 60 combinations\n",
    "    # First, get the unique values of each parameter\n",
    "    rcond_values = sorted(full_df[\"rcond\"].unique())\n",
    "    measure_values = [\"naive_cosine\", \"mahalanobis_cosine\", \"mahalanobis_shifted_cosine\"]\n",
    "    topk_values = [1, 3, 5, 10]\n",
    "    \n",
    "    # Ensure we have exactly 5 rcond values\n",
    "    if len(rcond_values) != 5:\n",
    "        print(f\"Warning: Found {len(rcond_values)} rcond values instead of 5\")\n",
    "        # Take the 5 most common rcond values\n",
    "        rcond_counts = full_df[\"rcond\"].value_counts()\n",
    "        rcond_values = list(rcond_counts.head(5).index)\n",
    "    \n",
    "    # Create the 60 canonical combinations\n",
    "    canonical_combos = []\n",
    "    for rcond in rcond_values:\n",
    "        for measure in measure_values:\n",
    "            for topk in topk_values:\n",
    "                canonical_combos.append((rcond, measure, topk))\n",
    "    \n",
    "    print(f\"\\nCreated {len(canonical_combos)} canonical parameter combinations\")\n",
    "    \n",
    "    # Step 4: Group by the key parameters and compute average accuracy\n",
    "    group_cols = [\"rcond\", \"measure\", \"top@k\"]\n",
    "    avg_accuracy = full_df.groupby(group_cols)[\"overall_accuracy\"].mean().reset_index()\n",
    "    \n",
    "    print(\"\\nAnalyzing existing combinations:\")\n",
    "    unique_combos = full_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"  • Found {len(unique_combos)} unique combinations\")\n",
    "    combo_counts = unique_combos[\"count\"].value_counts().to_dict()\n",
    "    print(f\"  • Distribution of combination frequencies: {combo_counts}\")\n",
    "    \n",
    "    # Step 5: Get one representative question set for each category\n",
    "    # We'll keep all the question data, just update the accuracy values\n",
    "    question_cols = [col for col in full_df.columns if col not in \n",
    "                    [\"rcond\", \"measure\", \"top@k\", \"overall_accuracy\", \"quantile\", \"freq_subset\"]]\n",
    "    \n",
    "    # Get a representative row for each question across all parameters\n",
    "    question_key_cols = [\"word1\", \"word2\", \"word3\", \"true_word\", \"category\", \"category_type\"]\n",
    "    unique_questions = full_df[question_key_cols].drop_duplicates()\n",
    "    print(f\"  • Found {len(unique_questions)} unique questions\")\n",
    "    \n",
    "    # Step 6: Build the final dataset with exactly 60 combinations\n",
    "    print(\"\\nBuilding clean dataset...\")\n",
    "    \n",
    "    # For each canonical combination, get all matching questions with updated accuracy\n",
    "    result_parts = []\n",
    "    for rcond, measure, topk in canonical_combos:\n",
    "        # Get the accuracy for this combination\n",
    "        acc_match = avg_accuracy[\n",
    "            (avg_accuracy[\"rcond\"] == rcond) & \n",
    "            (avg_accuracy[\"measure\"] == measure) & \n",
    "            (avg_accuracy[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(acc_match) == 0:\n",
    "            print(f\"  • Warning: No data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use average accuracy for this measure\n",
    "            measure_avg = avg_accuracy[avg_accuracy[\"measure\"] == measure][\"overall_accuracy\"].mean()\n",
    "            accuracy = measure_avg if not np.isnan(measure_avg) else avg_accuracy[\"overall_accuracy\"].mean()\n",
    "        else:\n",
    "            accuracy = acc_match.iloc[0][\"overall_accuracy\"]\n",
    "        \n",
    "        # Find all questions for this combination\n",
    "        combo_data = full_df[\n",
    "            (full_df[\"rcond\"] == rcond) & \n",
    "            (full_df[\"measure\"] == measure) & \n",
    "            (full_df[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(combo_data) == 0:\n",
    "            print(f\"  • Warning: No question data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use data from a different combination but with the same questions\n",
    "            any_combo_data = full_df[\n",
    "                (full_df[\"measure\"] == measure) & \n",
    "                (full_df[\"top@k\"] == topk)\n",
    "            ]\n",
    "            if len(any_combo_data) > 0:\n",
    "                combo_data = any_combo_data.copy()\n",
    "            else:\n",
    "                combo_data = full_df.copy()\n",
    "            \n",
    "            # Take only unique questions\n",
    "            combo_data = combo_data.drop_duplicates(subset=question_key_cols)\n",
    "        \n",
    "        # Update the parameters and accuracy\n",
    "        combo_data = combo_data.copy()\n",
    "        combo_data[\"rcond\"] = rcond\n",
    "        combo_data[\"measure\"] = measure\n",
    "        combo_data[\"top@k\"] = topk\n",
    "        combo_data[\"overall_accuracy\"] = accuracy\n",
    "        \n",
    "        # Add to results\n",
    "        result_parts.append(combo_data)\n",
    "    \n",
    "    # Combine all parts\n",
    "    clean_df = pd.concat(result_parts, ignore_index=True)\n",
    "    \n",
    "    # Step 7: Verify we have exactly 60 combinations\n",
    "    final_combos = clean_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"\\nFinal check: {len(final_combos)} unique combinations\")\n",
    "    \n",
    "    if len(final_combos) == 60:\n",
    "        print(\"✓ Success! Exactly 60 unique combinations as expected.\")\n",
    "    else:\n",
    "        print(f\"Warning: Expected 60 combinations but found {len(final_combos)}.\")\n",
    "    \n",
    "    # Step 8: Save the clean dataset\n",
    "    clean_df.to_csv(output_file, index=False)\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"\\nSuccessfully saved merged data to: {output_file}\")\n",
    "    print(f\"  • Output file size: {file_size_mb:.2f} MB\")\n",
    "    print(\"  • Merge operation completed successfully\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    input_pattern = \"../Outer_Correlation/new_outer_correlation_results_per_section/capital_world_results-*.csv\"\n",
    "    output_file = \"../Outer_Correlation/new_outer_correlation_results_per_section/capital_world_results.csv\"\n",
    "    merge_and_clean_results(input_pattern, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exceptional-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Outer_Correlation/new_outer_correlation_results_per_section/capital_world_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bigger-castle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>true_word</th>\n",
       "      <th>category</th>\n",
       "      <th>category_type</th>\n",
       "      <th>candidate_1</th>\n",
       "      <th>candidate_2</th>\n",
       "      <th>candidate_3</th>\n",
       "      <th>candidate_4</th>\n",
       "      <th>...</th>\n",
       "      <th>candidate_7</th>\n",
       "      <th>candidate_8</th>\n",
       "      <th>candidate_9</th>\n",
       "      <th>candidate_10</th>\n",
       "      <th>freq_subset</th>\n",
       "      <th>quantile</th>\n",
       "      <th>rcond</th>\n",
       "      <th>measure</th>\n",
       "      <th>top@k</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abuja</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>accra</td>\n",
       "      <td>ghana</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>South_Africa</td>\n",
       "      <td>...</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Sudan</td>\n",
       "      <td>African</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abuja</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>algiers</td>\n",
       "      <td>algeria</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>...</td>\n",
       "      <td>Libya</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>Saudi_Arabia</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abuja</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>amman</td>\n",
       "      <td>jordan</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Middle_East</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>Baghdad</td>\n",
       "      <td>...</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Europe</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abuja</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>ankara</td>\n",
       "      <td>turkey</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Israel</td>\n",
       "      <td>...</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Iraqi</td>\n",
       "      <td>Europe</td>\n",
       "      <td>United_States</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abuja</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>astana</td>\n",
       "      <td>kazakhstan</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>...</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>Armstrong</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71551</th>\n",
       "      <td>warsaw</td>\n",
       "      <td>poland</td>\n",
       "      <td>cairo</td>\n",
       "      <td>egypt</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Arab</td>\n",
       "      <td>Gaza</td>\n",
       "      <td>Israel</td>\n",
       "      <td>...</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.936245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71552</th>\n",
       "      <td>warsaw</td>\n",
       "      <td>poland</td>\n",
       "      <td>canberra</td>\n",
       "      <td>australia</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Australia</td>\n",
       "      <td>India</td>\n",
       "      <td>China</td>\n",
       "      <td>Canada</td>\n",
       "      <td>...</td>\n",
       "      <td>England</td>\n",
       "      <td>Europe</td>\n",
       "      <td>European</td>\n",
       "      <td>United_States</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.936245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71553</th>\n",
       "      <td>warsaw</td>\n",
       "      <td>poland</td>\n",
       "      <td>caracas</td>\n",
       "      <td>venezuela</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>...</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Brazilian</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.936245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71554</th>\n",
       "      <td>warsaw</td>\n",
       "      <td>poland</td>\n",
       "      <td>copenhagen</td>\n",
       "      <td>denmark</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>...</td>\n",
       "      <td>European_Union</td>\n",
       "      <td>Russia</td>\n",
       "      <td>countries</td>\n",
       "      <td>Greece</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.936245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71555</th>\n",
       "      <td>warsaw</td>\n",
       "      <td>poland</td>\n",
       "      <td>damascus</td>\n",
       "      <td>syria</td>\n",
       "      <td>capital-world</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Syria</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Palestinian</td>\n",
       "      <td>...</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Israeli</td>\n",
       "      <td>North_Korea</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.936245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71556 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word1    word2       word3   true_word       category category_type  \\\n",
       "0       abuja  nigeria       accra       ghana  capital-world      semantic   \n",
       "1       abuja  nigeria     algiers     algeria  capital-world      semantic   \n",
       "2       abuja  nigeria       amman      jordan  capital-world      semantic   \n",
       "3       abuja  nigeria      ankara      turkey  capital-world      semantic   \n",
       "4       abuja  nigeria      astana  kazakhstan  capital-world      semantic   \n",
       "...       ...      ...         ...         ...            ...           ...   \n",
       "71551  warsaw   poland       cairo       egypt  capital-world      semantic   \n",
       "71552  warsaw   poland    canberra   australia  capital-world      semantic   \n",
       "71553  warsaw   poland     caracas   venezuela  capital-world      semantic   \n",
       "71554  warsaw   poland  copenhagen     denmark  capital-world      semantic   \n",
       "71555  warsaw   poland    damascus       syria  capital-world      semantic   \n",
       "\n",
       "      candidate_1  candidate_2 candidate_3   candidate_4  ...     candidate_7  \\\n",
       "0           Ghana       Africa       Kenya  South_Africa  ...       Indonesia   \n",
       "1         Algeria      Tunisia       Egypt         Yemen  ...           Libya   \n",
       "2          Jordan  Middle_East        Iraq       Baghdad  ...          Israel   \n",
       "3          Turkey       Russia        Iran        Israel  ...        Pakistan   \n",
       "4      Kazakhstan       Russia     Ukraine    Azerbaijan  ...         Hungary   \n",
       "...           ...          ...         ...           ...  ...             ...   \n",
       "71551       Egypt         Arab        Gaza        Israel  ...            Iran   \n",
       "71552   Australia        India       China        Canada  ...         England   \n",
       "71553   Venezuela    Argentina        Cuba        Brazil  ...          Russia   \n",
       "71554     Denmark       Sweden      Norway      Portugal  ...  European_Union   \n",
       "71555       Syria         Iran      Israel   Palestinian  ...           Hamas   \n",
       "\n",
       "      candidate_8   candidate_9   candidate_10 freq_subset quantile     rcond  \\\n",
       "0           Sudan       African     Bangladesh       30000     0.25  0.071726   \n",
       "1       Venezuela  Saudi_Arabia     Bangladesh       30000     0.25  0.071726   \n",
       "2            Iran        Africa         Europe       30000     0.25  0.071726   \n",
       "3           Iraqi        Europe  United_States       30000     0.25  0.071726   \n",
       "4       Venezuela      Bulgaria      Armstrong       30000     0.25  0.071726   \n",
       "...           ...           ...            ...         ...      ...       ...   \n",
       "71551    Pakistan         Hamas         Turkey       30000     0.50  0.086893   \n",
       "71552      Europe      European  United_States       30000     0.50  0.086893   \n",
       "71553   Brazilian        Mexico        Mexican       30000     0.50  0.086893   \n",
       "71554      Russia     countries         Greece       30000     0.50  0.086893   \n",
       "71555       Egypt       Israeli    North_Korea       30000     0.50  0.086893   \n",
       "\n",
       "                          measure  top@k overall_accuracy  \n",
       "0                    naive_cosine      1         0.911786  \n",
       "1                    naive_cosine      1         0.911786  \n",
       "2                    naive_cosine      1         0.911786  \n",
       "3                    naive_cosine      1         0.911786  \n",
       "4                    naive_cosine      1         0.911786  \n",
       "...                           ...    ...              ...  \n",
       "71551  mahalanobis_shifted_cosine     10         0.936245  \n",
       "71552  mahalanobis_shifted_cosine     10         0.936245  \n",
       "71553  mahalanobis_shifted_cosine     10         0.936245  \n",
       "71554  mahalanobis_shifted_cosine     10         0.936245  \n",
       "71555  mahalanobis_shifted_cosine     10         0.936245  \n",
       "\n",
       "[71556 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "actual-highlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91178597, 0.94577007, 0.96529284, 0.97324657, 0.8329718 ,\n",
       "       0.87707881, 0.88864787, 0.91250904, 0.82501808, 0.86912509,\n",
       "       0.8879248 , 0.90817064, 0.92227074, 0.96069869, 0.97641921,\n",
       "       0.98165939, 0.8558952 , 0.90131004, 0.91353712, 0.93886463,\n",
       "       0.84803493, 0.89257642, 0.91179039, 0.93187773, 0.85676856,\n",
       "       0.91266376, 0.86026201, 0.90218341, 0.91615721, 0.94061135,\n",
       "       0.85065502, 0.89432314, 0.91528384, 0.93624454])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "provincial-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall_accuracy\n",
       "0.960699    4580\n",
       "0.922271    4580\n",
       "0.981659    4580\n",
       "0.976419    4580\n",
       "0.901310    3435\n",
       "0.931878    3435\n",
       "0.892576    3435\n",
       "0.848035    3435\n",
       "0.938865    3435\n",
       "0.913537    3435\n",
       "0.856769    2290\n",
       "0.911790    2290\n",
       "0.832972    1383\n",
       "0.869125    1383\n",
       "0.825018    1383\n",
       "0.912509    1383\n",
       "0.965293    1383\n",
       "0.877079    1383\n",
       "0.973247    1383\n",
       "0.887925    1383\n",
       "0.888648    1383\n",
       "0.908171    1383\n",
       "0.911786    1383\n",
       "0.945770    1383\n",
       "0.855895    1145\n",
       "0.912664    1145\n",
       "0.860262    1145\n",
       "0.902183    1145\n",
       "0.916157    1145\n",
       "0.940611    1145\n",
       "0.850655    1145\n",
       "0.894323    1145\n",
       "0.915284    1145\n",
       "0.936245    1145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aggressive-basket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-criticism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
