{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bridal-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/3/hassa940/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging files matching pattern: ../Outer_Correlation/new_outer_correlation_results_per_section/gram3_comparative_results-*.csv\n",
      "Found 2 files:\n",
      "  • Found gram3_comparative_results-1.csv\n",
      "  • Found gram3_comparative_results-2.csv\n",
      "\n",
      "Concatenating all files...\n",
      "  • Combined data: 79920 rows\n",
      "\n",
      "Created 60 canonical parameter combinations\n",
      "\n",
      "Analyzing existing combinations:\n",
      "  • Found 60 unique combinations\n",
      "  • Distribution of combination frequencies: {1332: 60}\n",
      "  • Found 1332 unique questions\n",
      "\n",
      "Building clean dataset...\n",
      "\n",
      "Final check: 60 unique combinations\n",
      "✓ Success! Exactly 60 unique combinations as expected.\n",
      "\n",
      "Successfully saved merged data to: ../Outer_Correlation/new_outer_correlation_results_per_section/gram3_comparative_results.csv\n",
      "  • Output file size: 14.82 MB\n",
      "  • Merge operation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_and_clean_results(input_pattern, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files and enforce exactly 60 combinations by:\n",
    "    1. Identifying the canonical 60 combinations (5 rcond × 3 measures × 4 top@k)\n",
    "    2. Grouping by these combinations and averaging the accuracy values\n",
    "    3. Creating a clean dataset with exactly these combinations\n",
    "    \n",
    "    Args:\n",
    "        input_pattern: Glob pattern to match input files (e.g., \"*.csv\")\n",
    "        output_file: Path to save the merged and cleaned file\n",
    "    \"\"\"\n",
    "    print(f\"Merging files matching pattern: {input_pattern}\")\n",
    "    \n",
    "    # Step 1: Load all CSV files\n",
    "    all_files = sorted(glob.glob(input_pattern))\n",
    "    if not all_files:\n",
    "        print(f\"Error: No files found matching {input_pattern}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files:\")\n",
    "    for file in all_files:\n",
    "        print(f\"  • Found {os.path.basename(file)}\")\n",
    "    \n",
    "    # Load each file\n",
    "    all_dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Step 2: Concatenate all files\n",
    "    print(\"\\nConcatenating all files...\")\n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"  • Combined data: {len(full_df)} rows\")\n",
    "    \n",
    "    # Step 3: Identify the canonical 60 combinations\n",
    "    # First, get the unique values of each parameter\n",
    "    rcond_values = sorted(full_df[\"rcond\"].unique())\n",
    "    measure_values = [\"naive_cosine\", \"mahalanobis_cosine\", \"mahalanobis_shifted_cosine\"]\n",
    "    topk_values = [1, 3, 5, 10]\n",
    "    \n",
    "    # Ensure we have exactly 5 rcond values\n",
    "    if len(rcond_values) != 5:\n",
    "        print(f\"Warning: Found {len(rcond_values)} rcond values instead of 5\")\n",
    "        # Take the 5 most common rcond values\n",
    "        rcond_counts = full_df[\"rcond\"].value_counts()\n",
    "        rcond_values = list(rcond_counts.head(5).index)\n",
    "    \n",
    "    # Create the 60 canonical combinations\n",
    "    canonical_combos = []\n",
    "    for rcond in rcond_values:\n",
    "        for measure in measure_values:\n",
    "            for topk in topk_values:\n",
    "                canonical_combos.append((rcond, measure, topk))\n",
    "    \n",
    "    print(f\"\\nCreated {len(canonical_combos)} canonical parameter combinations\")\n",
    "    \n",
    "    # Step 4: Group by the key parameters and compute average accuracy\n",
    "    group_cols = [\"rcond\", \"measure\", \"top@k\"]\n",
    "    avg_accuracy = full_df.groupby(group_cols)[\"overall_accuracy\"].mean().reset_index()\n",
    "    \n",
    "    print(\"\\nAnalyzing existing combinations:\")\n",
    "    unique_combos = full_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"  • Found {len(unique_combos)} unique combinations\")\n",
    "    combo_counts = unique_combos[\"count\"].value_counts().to_dict()\n",
    "    print(f\"  • Distribution of combination frequencies: {combo_counts}\")\n",
    "    \n",
    "    # Step 5: Get one representative question set for each category\n",
    "    # We'll keep all the question data, just update the accuracy values\n",
    "    question_cols = [col for col in full_df.columns if col not in \n",
    "                    [\"rcond\", \"measure\", \"top@k\", \"overall_accuracy\", \"quantile\", \"freq_subset\"]]\n",
    "    \n",
    "    # Get a representative row for each question across all parameters\n",
    "    question_key_cols = [\"word1\", \"word2\", \"word3\", \"true_word\", \"category\", \"category_type\"]\n",
    "    unique_questions = full_df[question_key_cols].drop_duplicates()\n",
    "    print(f\"  • Found {len(unique_questions)} unique questions\")\n",
    "    \n",
    "    # Step 6: Build the final dataset with exactly 60 combinations\n",
    "    print(\"\\nBuilding clean dataset...\")\n",
    "    \n",
    "    # For each canonical combination, get all matching questions with updated accuracy\n",
    "    result_parts = []\n",
    "    for rcond, measure, topk in canonical_combos:\n",
    "        # Get the accuracy for this combination\n",
    "        acc_match = avg_accuracy[\n",
    "            (avg_accuracy[\"rcond\"] == rcond) & \n",
    "            (avg_accuracy[\"measure\"] == measure) & \n",
    "            (avg_accuracy[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(acc_match) == 0:\n",
    "            print(f\"  • Warning: No data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use average accuracy for this measure\n",
    "            measure_avg = avg_accuracy[avg_accuracy[\"measure\"] == measure][\"overall_accuracy\"].mean()\n",
    "            accuracy = measure_avg if not np.isnan(measure_avg) else avg_accuracy[\"overall_accuracy\"].mean()\n",
    "        else:\n",
    "            accuracy = acc_match.iloc[0][\"overall_accuracy\"]\n",
    "        \n",
    "        # Find all questions for this combination\n",
    "        combo_data = full_df[\n",
    "            (full_df[\"rcond\"] == rcond) & \n",
    "            (full_df[\"measure\"] == measure) & \n",
    "            (full_df[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(combo_data) == 0:\n",
    "            print(f\"  • Warning: No question data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use data from a different combination but with the same questions\n",
    "            any_combo_data = full_df[\n",
    "                (full_df[\"measure\"] == measure) & \n",
    "                (full_df[\"top@k\"] == topk)\n",
    "            ]\n",
    "            if len(any_combo_data) > 0:\n",
    "                combo_data = any_combo_data.copy()\n",
    "            else:\n",
    "                combo_data = full_df.copy()\n",
    "            \n",
    "            # Take only unique questions\n",
    "            combo_data = combo_data.drop_duplicates(subset=question_key_cols)\n",
    "        \n",
    "        # Update the parameters and accuracy\n",
    "        combo_data = combo_data.copy()\n",
    "        combo_data[\"rcond\"] = rcond\n",
    "        combo_data[\"measure\"] = measure\n",
    "        combo_data[\"top@k\"] = topk\n",
    "        combo_data[\"overall_accuracy\"] = accuracy\n",
    "        \n",
    "        # Add to results\n",
    "        result_parts.append(combo_data)\n",
    "    \n",
    "    # Combine all parts\n",
    "    clean_df = pd.concat(result_parts, ignore_index=True)\n",
    "    \n",
    "    # Step 7: Verify we have exactly 60 combinations\n",
    "    final_combos = clean_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"\\nFinal check: {len(final_combos)} unique combinations\")\n",
    "    \n",
    "    if len(final_combos) == 60:\n",
    "        print(\"✓ Success! Exactly 60 unique combinations as expected.\")\n",
    "    else:\n",
    "        print(f\"Warning: Expected 60 combinations but found {len(final_combos)}.\")\n",
    "    \n",
    "    # Step 8: Save the clean dataset\n",
    "    clean_df.to_csv(output_file, index=False)\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"\\nSuccessfully saved merged data to: {output_file}\")\n",
    "    print(f\"  • Output file size: {file_size_mb:.2f} MB\")\n",
    "    print(\"  • Merge operation completed successfully\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    input_pattern = \"../Outer_Correlation/new_outer_correlation_results_per_section/gram3_comparative_results-*.csv\"\n",
    "    output_file = \"../Outer_Correlation/new_outer_correlation_results_per_section/gram3_comparative_results.csv\"\n",
    "    merge_and_clean_results(input_pattern, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interpreted-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Outer_Correlation/new_outer_correlation_results_per_section/gram3_comparative_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bizarre-compression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>true_word</th>\n",
       "      <th>category</th>\n",
       "      <th>category_type</th>\n",
       "      <th>candidate_1</th>\n",
       "      <th>candidate_2</th>\n",
       "      <th>candidate_3</th>\n",
       "      <th>candidate_4</th>\n",
       "      <th>...</th>\n",
       "      <th>candidate_7</th>\n",
       "      <th>candidate_8</th>\n",
       "      <th>candidate_9</th>\n",
       "      <th>candidate_10</th>\n",
       "      <th>freq_subset</th>\n",
       "      <th>quantile</th>\n",
       "      <th>rcond</th>\n",
       "      <th>measure</th>\n",
       "      <th>top@k</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad</td>\n",
       "      <td>worse</td>\n",
       "      <td>big</td>\n",
       "      <td>bigger</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>bigger</td>\n",
       "      <td>larger</td>\n",
       "      <td>biggest</td>\n",
       "      <td>smaller</td>\n",
       "      <td>...</td>\n",
       "      <td>major</td>\n",
       "      <td>better</td>\n",
       "      <td>closer</td>\n",
       "      <td>more</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>worse</td>\n",
       "      <td>bright</td>\n",
       "      <td>brighter</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>brighter</td>\n",
       "      <td>bleak</td>\n",
       "      <td>better</td>\n",
       "      <td>happier</td>\n",
       "      <td>...</td>\n",
       "      <td>blue</td>\n",
       "      <td>horizon</td>\n",
       "      <td>dire</td>\n",
       "      <td>cleaner</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bad</td>\n",
       "      <td>worse</td>\n",
       "      <td>cheap</td>\n",
       "      <td>cheaper</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>cheaper</td>\n",
       "      <td>expensive</td>\n",
       "      <td>less</td>\n",
       "      <td>affordable</td>\n",
       "      <td>...</td>\n",
       "      <td>even</td>\n",
       "      <td>harder</td>\n",
       "      <td>better</td>\n",
       "      <td>easier</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>worse</td>\n",
       "      <td>cold</td>\n",
       "      <td>colder</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>colder</td>\n",
       "      <td>warmer</td>\n",
       "      <td>chilly</td>\n",
       "      <td>cooler</td>\n",
       "      <td>...</td>\n",
       "      <td>icy</td>\n",
       "      <td>chill</td>\n",
       "      <td>winter</td>\n",
       "      <td>wet</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad</td>\n",
       "      <td>worse</td>\n",
       "      <td>cool</td>\n",
       "      <td>cooler</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>cooler</td>\n",
       "      <td>cooling</td>\n",
       "      <td>better</td>\n",
       "      <td>more</td>\n",
       "      <td>...</td>\n",
       "      <td>calm</td>\n",
       "      <td>heat</td>\n",
       "      <td>harder</td>\n",
       "      <td>warm</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79915</th>\n",
       "      <td>young</td>\n",
       "      <td>younger</td>\n",
       "      <td>tight</td>\n",
       "      <td>tighter</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>tighter</td>\n",
       "      <td>comfortable</td>\n",
       "      <td>shorter</td>\n",
       "      <td>tightening</td>\n",
       "      <td>...</td>\n",
       "      <td>tougher</td>\n",
       "      <td>relaxed</td>\n",
       "      <td>tough</td>\n",
       "      <td>stronger</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79916</th>\n",
       "      <td>young</td>\n",
       "      <td>younger</td>\n",
       "      <td>tough</td>\n",
       "      <td>tougher</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>tougher</td>\n",
       "      <td>harder</td>\n",
       "      <td>difficult</td>\n",
       "      <td>better</td>\n",
       "      <td>...</td>\n",
       "      <td>hard</td>\n",
       "      <td>easier</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>rough</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79917</th>\n",
       "      <td>young</td>\n",
       "      <td>younger</td>\n",
       "      <td>warm</td>\n",
       "      <td>warmer</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>warmer</td>\n",
       "      <td>cooler</td>\n",
       "      <td>warming</td>\n",
       "      <td>cool</td>\n",
       "      <td>...</td>\n",
       "      <td>pleasant</td>\n",
       "      <td>relaxing</td>\n",
       "      <td>wet</td>\n",
       "      <td>relax</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79918</th>\n",
       "      <td>young</td>\n",
       "      <td>younger</td>\n",
       "      <td>weak</td>\n",
       "      <td>weaker</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>weaker</td>\n",
       "      <td>weakness</td>\n",
       "      <td>stronger</td>\n",
       "      <td>lower</td>\n",
       "      <td>...</td>\n",
       "      <td>negative</td>\n",
       "      <td>strong</td>\n",
       "      <td>favorable</td>\n",
       "      <td>decline</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79919</th>\n",
       "      <td>young</td>\n",
       "      <td>younger</td>\n",
       "      <td>wide</td>\n",
       "      <td>wider</td>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>wider</td>\n",
       "      <td>smaller</td>\n",
       "      <td>larger</td>\n",
       "      <td>bigger</td>\n",
       "      <td>...</td>\n",
       "      <td>within</td>\n",
       "      <td>broad</td>\n",
       "      <td>combined</td>\n",
       "      <td>consistent</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79920 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word1    word2   word3 true_word           category category_type  \\\n",
       "0        bad    worse     big    bigger  gram3-comparative     syntactic   \n",
       "1        bad    worse  bright  brighter  gram3-comparative     syntactic   \n",
       "2        bad    worse   cheap   cheaper  gram3-comparative     syntactic   \n",
       "3        bad    worse    cold    colder  gram3-comparative     syntactic   \n",
       "4        bad    worse    cool    cooler  gram3-comparative     syntactic   \n",
       "...      ...      ...     ...       ...                ...           ...   \n",
       "79915  young  younger   tight   tighter  gram3-comparative     syntactic   \n",
       "79916  young  younger   tough   tougher  gram3-comparative     syntactic   \n",
       "79917  young  younger    warm    warmer  gram3-comparative     syntactic   \n",
       "79918  young  younger    weak    weaker  gram3-comparative     syntactic   \n",
       "79919  young  younger    wide     wider  gram3-comparative     syntactic   \n",
       "\n",
       "      candidate_1  candidate_2 candidate_3 candidate_4  ... candidate_7  \\\n",
       "0          bigger       larger     biggest     smaller  ...       major   \n",
       "1        brighter        bleak      better     happier  ...        blue   \n",
       "2         cheaper    expensive        less  affordable  ...        even   \n",
       "3          colder       warmer      chilly      cooler  ...         icy   \n",
       "4          cooler      cooling      better        more  ...        calm   \n",
       "...           ...          ...         ...         ...  ...         ...   \n",
       "79915     tighter  comfortable     shorter  tightening  ...     tougher   \n",
       "79916     tougher       harder   difficult      better  ...        hard   \n",
       "79917      warmer       cooler     warming        cool  ...    pleasant   \n",
       "79918      weaker     weakness    stronger       lower  ...    negative   \n",
       "79919       wider      smaller      larger      bigger  ...      within   \n",
       "\n",
       "      candidate_8 candidate_9 candidate_10 freq_subset quantile     rcond  \\\n",
       "0          better      closer         more       30000     0.01  0.053319   \n",
       "1         horizon        dire      cleaner       30000     0.01  0.053319   \n",
       "2          harder      better       easier       30000     0.01  0.053319   \n",
       "3           chill      winter          wet       30000     0.01  0.053319   \n",
       "4            heat      harder         warm       30000     0.01  0.053319   \n",
       "...           ...         ...          ...         ...      ...       ...   \n",
       "79915     relaxed       tough     stronger       30000     0.50  0.086893   \n",
       "79916      easier  aggressive        rough       30000     0.50  0.086893   \n",
       "79917    relaxing         wet        relax       30000     0.50  0.086893   \n",
       "79918      strong   favorable      decline       30000     0.50  0.086893   \n",
       "79919       broad    combined   consistent       30000     0.50  0.086893   \n",
       "\n",
       "                          measure  top@k overall_accuracy  \n",
       "0                    naive_cosine      1         0.939940  \n",
       "1                    naive_cosine      1         0.939940  \n",
       "2                    naive_cosine      1         0.939940  \n",
       "3                    naive_cosine      1         0.939940  \n",
       "4                    naive_cosine      1         0.939940  \n",
       "...                           ...    ...              ...  \n",
       "79915  mahalanobis_shifted_cosine     10         0.978228  \n",
       "79916  mahalanobis_shifted_cosine     10         0.978228  \n",
       "79917  mahalanobis_shifted_cosine     10         0.978228  \n",
       "79918  mahalanobis_shifted_cosine     10         0.978228  \n",
       "79919  mahalanobis_shifted_cosine     10         0.978228  \n",
       "\n",
       "[79920 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "academic-scholar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93993994, 0.97972973, 0.98873874, 0.99624625, 0.89864865,\n",
       "       0.94369369, 0.95570571, 0.96996997, 0.89714715, 0.94444444,\n",
       "       0.95345345, 0.96846847, 0.95645646, 0.9527027 , 0.96921922,\n",
       "       0.90015015, 0.97072072, 0.8978979 , 0.90465465, 0.9466967 ,\n",
       "       0.95795796, 0.97522523, 0.9466967 , 0.97372372, 0.90765766,\n",
       "       0.96471471, 0.97822823, 0.90690691, 0.9512012 , 0.96171171])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latter-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall_accuracy\n",
       "0.939940    6660\n",
       "0.996246    6660\n",
       "0.979730    6660\n",
       "0.944444    6660\n",
       "0.988739    6660\n",
       "0.956456    3996\n",
       "0.969970    2664\n",
       "0.897147    2664\n",
       "0.898649    2664\n",
       "0.900150    2664\n",
       "0.952703    2664\n",
       "0.969219    2664\n",
       "0.978228    2664\n",
       "0.953453    2664\n",
       "0.975225    1332\n",
       "0.955706    1332\n",
       "0.943694    1332\n",
       "0.970721    1332\n",
       "0.897898    1332\n",
       "0.904655    1332\n",
       "0.946697    1332\n",
       "0.946697    1332\n",
       "0.957958    1332\n",
       "0.973724    1332\n",
       "0.907658    1332\n",
       "0.964715    1332\n",
       "0.906907    1332\n",
       "0.951201    1332\n",
       "0.968468    1332\n",
       "0.961712    1332\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "overhead-holly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-bailey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
