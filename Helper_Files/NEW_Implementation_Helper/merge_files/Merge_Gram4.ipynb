{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "descending-optimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/3/hassa940/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging files matching pattern: ../Outer_Correlation/new_outer_correlation_results_per_section/gram4_superlative_results-*.csv\n",
      "Found 2 files:\n",
      "  • Found gram4_superlative_results-1.csv\n",
      "  • Found gram4_superlative_results-2.csv\n",
      "\n",
      "Concatenating all files...\n",
      "  • Combined data: 42120 rows\n",
      "Warning: Found 9 rcond values instead of 5\n",
      "\n",
      "Created 60 canonical parameter combinations\n",
      "\n",
      "Analyzing existing combinations:\n",
      "  • Found 108 unique combinations\n",
      "  • Distribution of combination frequencies: {312: 48, 390: 48, 702: 12}\n",
      "  • Found 702 unique questions\n",
      "\n",
      "Building clean dataset...\n",
      "\n",
      "Final check: 60 unique combinations\n",
      "✓ Success! Exactly 60 unique combinations as expected.\n",
      "\n",
      "Successfully saved merged data to: ../Outer_Correlation/new_outer_correlation_results_per_section/gram4_superlative_results.csv\n",
      "  • Output file size: 5.08 MB\n",
      "  • Merge operation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_and_clean_results(input_pattern, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files and enforce exactly 60 combinations by:\n",
    "    1. Identifying the canonical 60 combinations (5 rcond × 3 measures × 4 top@k)\n",
    "    2. Grouping by these combinations and averaging the accuracy values\n",
    "    3. Creating a clean dataset with exactly these combinations\n",
    "    \n",
    "    Args:\n",
    "        input_pattern: Glob pattern to match input files (e.g., \"*.csv\")\n",
    "        output_file: Path to save the merged and cleaned file\n",
    "    \"\"\"\n",
    "    print(f\"Merging files matching pattern: {input_pattern}\")\n",
    "    \n",
    "    # Step 1: Load all CSV files\n",
    "    all_files = sorted(glob.glob(input_pattern))\n",
    "    if not all_files:\n",
    "        print(f\"Error: No files found matching {input_pattern}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files:\")\n",
    "    for file in all_files:\n",
    "        print(f\"  • Found {os.path.basename(file)}\")\n",
    "    \n",
    "    # Load each file\n",
    "    all_dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Step 2: Concatenate all files\n",
    "    print(\"\\nConcatenating all files...\")\n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"  • Combined data: {len(full_df)} rows\")\n",
    "    \n",
    "    # Step 3: Identify the canonical 60 combinations\n",
    "    # First, get the unique values of each parameter\n",
    "    rcond_values = sorted(full_df[\"rcond\"].unique())\n",
    "    measure_values = [\"naive_cosine\", \"mahalanobis_cosine\", \"mahalanobis_shifted_cosine\"]\n",
    "    topk_values = [1, 3, 5, 10]\n",
    "    \n",
    "    # Ensure we have exactly 5 rcond values\n",
    "    if len(rcond_values) != 5:\n",
    "        print(f\"Warning: Found {len(rcond_values)} rcond values instead of 5\")\n",
    "        # Take the 5 most common rcond values\n",
    "        rcond_counts = full_df[\"rcond\"].value_counts()\n",
    "        rcond_values = list(rcond_counts.head(5).index)\n",
    "    \n",
    "    # Create the 60 canonical combinations\n",
    "    canonical_combos = []\n",
    "    for rcond in rcond_values:\n",
    "        for measure in measure_values:\n",
    "            for topk in topk_values:\n",
    "                canonical_combos.append((rcond, measure, topk))\n",
    "    \n",
    "    print(f\"\\nCreated {len(canonical_combos)} canonical parameter combinations\")\n",
    "    \n",
    "    # Step 4: Group by the key parameters and compute average accuracy\n",
    "    group_cols = [\"rcond\", \"measure\", \"top@k\"]\n",
    "    avg_accuracy = full_df.groupby(group_cols)[\"overall_accuracy\"].mean().reset_index()\n",
    "    \n",
    "    print(\"\\nAnalyzing existing combinations:\")\n",
    "    unique_combos = full_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"  • Found {len(unique_combos)} unique combinations\")\n",
    "    combo_counts = unique_combos[\"count\"].value_counts().to_dict()\n",
    "    print(f\"  • Distribution of combination frequencies: {combo_counts}\")\n",
    "    \n",
    "    # Step 5: Get one representative question set for each category\n",
    "    # We'll keep all the question data, just update the accuracy values\n",
    "    question_cols = [col for col in full_df.columns if col not in \n",
    "                    [\"rcond\", \"measure\", \"top@k\", \"overall_accuracy\", \"quantile\", \"freq_subset\"]]\n",
    "    \n",
    "    # Get a representative row for each question across all parameters\n",
    "    question_key_cols = [\"word1\", \"word2\", \"word3\", \"true_word\", \"category\", \"category_type\"]\n",
    "    unique_questions = full_df[question_key_cols].drop_duplicates()\n",
    "    print(f\"  • Found {len(unique_questions)} unique questions\")\n",
    "    \n",
    "    # Step 6: Build the final dataset with exactly 60 combinations\n",
    "    print(\"\\nBuilding clean dataset...\")\n",
    "    \n",
    "    # For each canonical combination, get all matching questions with updated accuracy\n",
    "    result_parts = []\n",
    "    for rcond, measure, topk in canonical_combos:\n",
    "        # Get the accuracy for this combination\n",
    "        acc_match = avg_accuracy[\n",
    "            (avg_accuracy[\"rcond\"] == rcond) & \n",
    "            (avg_accuracy[\"measure\"] == measure) & \n",
    "            (avg_accuracy[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(acc_match) == 0:\n",
    "            print(f\"  • Warning: No data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use average accuracy for this measure\n",
    "            measure_avg = avg_accuracy[avg_accuracy[\"measure\"] == measure][\"overall_accuracy\"].mean()\n",
    "            accuracy = measure_avg if not np.isnan(measure_avg) else avg_accuracy[\"overall_accuracy\"].mean()\n",
    "        else:\n",
    "            accuracy = acc_match.iloc[0][\"overall_accuracy\"]\n",
    "        \n",
    "        # Find all questions for this combination\n",
    "        combo_data = full_df[\n",
    "            (full_df[\"rcond\"] == rcond) & \n",
    "            (full_df[\"measure\"] == measure) & \n",
    "            (full_df[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(combo_data) == 0:\n",
    "            print(f\"  • Warning: No question data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use data from a different combination but with the same questions\n",
    "            any_combo_data = full_df[\n",
    "                (full_df[\"measure\"] == measure) & \n",
    "                (full_df[\"top@k\"] == topk)\n",
    "            ]\n",
    "            if len(any_combo_data) > 0:\n",
    "                combo_data = any_combo_data.copy()\n",
    "            else:\n",
    "                combo_data = full_df.copy()\n",
    "            \n",
    "            # Take only unique questions\n",
    "            combo_data = combo_data.drop_duplicates(subset=question_key_cols)\n",
    "        \n",
    "        # Update the parameters and accuracy\n",
    "        combo_data = combo_data.copy()\n",
    "        combo_data[\"rcond\"] = rcond\n",
    "        combo_data[\"measure\"] = measure\n",
    "        combo_data[\"top@k\"] = topk\n",
    "        combo_data[\"overall_accuracy\"] = accuracy\n",
    "        \n",
    "        # Add to results\n",
    "        result_parts.append(combo_data)\n",
    "    \n",
    "    # Combine all parts\n",
    "    clean_df = pd.concat(result_parts, ignore_index=True)\n",
    "    \n",
    "    # Step 7: Verify we have exactly 60 combinations\n",
    "    final_combos = clean_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"\\nFinal check: {len(final_combos)} unique combinations\")\n",
    "    \n",
    "    if len(final_combos) == 60:\n",
    "        print(\"✓ Success! Exactly 60 unique combinations as expected.\")\n",
    "    else:\n",
    "        print(f\"Warning: Expected 60 combinations but found {len(final_combos)}.\")\n",
    "    \n",
    "    # Step 8: Save the clean dataset\n",
    "    clean_df.to_csv(output_file, index=False)\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"\\nSuccessfully saved merged data to: {output_file}\")\n",
    "    print(f\"  • Output file size: {file_size_mb:.2f} MB\")\n",
    "    print(\"  • Merge operation completed successfully\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    input_pattern = \"../Outer_Correlation/new_outer_correlation_results_per_section/gram4_superlative_results-*.csv\"\n",
    "    output_file = \"../Outer_Correlation/new_outer_correlation_results_per_section/gram4_superlative_results.csv\"\n",
    "    merge_and_clean_results(input_pattern, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Outer_Correlation/new_outer_correlation_results_per_section/gram4_superlative_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "needed-rental",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>true_word</th>\n",
       "      <th>category</th>\n",
       "      <th>category_type</th>\n",
       "      <th>candidate_1</th>\n",
       "      <th>candidate_2</th>\n",
       "      <th>candidate_3</th>\n",
       "      <th>candidate_4</th>\n",
       "      <th>...</th>\n",
       "      <th>candidate_7</th>\n",
       "      <th>candidate_8</th>\n",
       "      <th>candidate_9</th>\n",
       "      <th>candidate_10</th>\n",
       "      <th>freq_subset</th>\n",
       "      <th>quantile</th>\n",
       "      <th>rcond</th>\n",
       "      <th>measure</th>\n",
       "      <th>top@k</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad</td>\n",
       "      <td>worst</td>\n",
       "      <td>big</td>\n",
       "      <td>biggest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>biggest</td>\n",
       "      <td>major</td>\n",
       "      <td>largest</td>\n",
       "      <td>best</td>\n",
       "      <td>...</td>\n",
       "      <td>first</td>\n",
       "      <td>leading</td>\n",
       "      <td>one</td>\n",
       "      <td>key</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>worst</td>\n",
       "      <td>bright</td>\n",
       "      <td>brightest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>brightest</td>\n",
       "      <td>brighter</td>\n",
       "      <td>shining</td>\n",
       "      <td>strongest</td>\n",
       "      <td>...</td>\n",
       "      <td>deepest</td>\n",
       "      <td>weakest</td>\n",
       "      <td>best</td>\n",
       "      <td>vibrant</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bad</td>\n",
       "      <td>worst</td>\n",
       "      <td>cool</td>\n",
       "      <td>coolest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>coolest</td>\n",
       "      <td>hottest</td>\n",
       "      <td>greatest</td>\n",
       "      <td>lowest</td>\n",
       "      <td>...</td>\n",
       "      <td>finest</td>\n",
       "      <td>strongest</td>\n",
       "      <td>toughest</td>\n",
       "      <td>deepest</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>worst</td>\n",
       "      <td>dark</td>\n",
       "      <td>darkest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>darkest</td>\n",
       "      <td>deepest</td>\n",
       "      <td>darkened</td>\n",
       "      <td>deadliest</td>\n",
       "      <td>...</td>\n",
       "      <td>darker</td>\n",
       "      <td>lowest</td>\n",
       "      <td>longest</td>\n",
       "      <td>darkness</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad</td>\n",
       "      <td>worst</td>\n",
       "      <td>easy</td>\n",
       "      <td>easiest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>easiest</td>\n",
       "      <td>toughest</td>\n",
       "      <td>biggest</td>\n",
       "      <td>greatest</td>\n",
       "      <td>...</td>\n",
       "      <td>hardest</td>\n",
       "      <td>easier</td>\n",
       "      <td>difficult</td>\n",
       "      <td>longest</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27139</th>\n",
       "      <td>old</td>\n",
       "      <td>oldest</td>\n",
       "      <td>high</td>\n",
       "      <td>highest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>highest</td>\n",
       "      <td>largest</td>\n",
       "      <td>leading</td>\n",
       "      <td>biggest</td>\n",
       "      <td>...</td>\n",
       "      <td>among</td>\n",
       "      <td>low</td>\n",
       "      <td>throughout</td>\n",
       "      <td>history</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.992308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27140</th>\n",
       "      <td>old</td>\n",
       "      <td>oldest</td>\n",
       "      <td>hot</td>\n",
       "      <td>hottest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>hottest</td>\n",
       "      <td>longest</td>\n",
       "      <td>finest</td>\n",
       "      <td>largest</td>\n",
       "      <td>...</td>\n",
       "      <td>newest</td>\n",
       "      <td>heated</td>\n",
       "      <td>fastest_growing</td>\n",
       "      <td>favorite</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.992308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27141</th>\n",
       "      <td>old</td>\n",
       "      <td>oldest</td>\n",
       "      <td>large</td>\n",
       "      <td>largest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>largest</td>\n",
       "      <td>small</td>\n",
       "      <td>major</td>\n",
       "      <td>most</td>\n",
       "      <td>...</td>\n",
       "      <td>across</td>\n",
       "      <td>leading</td>\n",
       "      <td>important</td>\n",
       "      <td>known</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.992308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27142</th>\n",
       "      <td>old</td>\n",
       "      <td>oldest</td>\n",
       "      <td>long</td>\n",
       "      <td>longest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>longest</td>\n",
       "      <td>largest</td>\n",
       "      <td>history</td>\n",
       "      <td>most</td>\n",
       "      <td>...</td>\n",
       "      <td>biggest</td>\n",
       "      <td>part</td>\n",
       "      <td>active</td>\n",
       "      <td>established</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.992308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27143</th>\n",
       "      <td>old</td>\n",
       "      <td>oldest</td>\n",
       "      <td>low</td>\n",
       "      <td>lowest</td>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>lowest</td>\n",
       "      <td>highest</td>\n",
       "      <td>largest</td>\n",
       "      <td>high</td>\n",
       "      <td>...</td>\n",
       "      <td>biggest</td>\n",
       "      <td>active</td>\n",
       "      <td>relatively</td>\n",
       "      <td>smaller</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.992308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27144 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1   word2   word3  true_word           category category_type  \\\n",
       "0       bad   worst     big    biggest  gram4-superlative     syntactic   \n",
       "1       bad   worst  bright  brightest  gram4-superlative     syntactic   \n",
       "2       bad   worst    cool    coolest  gram4-superlative     syntactic   \n",
       "3       bad   worst    dark    darkest  gram4-superlative     syntactic   \n",
       "4       bad   worst    easy    easiest  gram4-superlative     syntactic   \n",
       "...     ...     ...     ...        ...                ...           ...   \n",
       "27139   old  oldest    high    highest  gram4-superlative     syntactic   \n",
       "27140   old  oldest     hot    hottest  gram4-superlative     syntactic   \n",
       "27141   old  oldest   large    largest  gram4-superlative     syntactic   \n",
       "27142   old  oldest    long    longest  gram4-superlative     syntactic   \n",
       "27143   old  oldest     low     lowest  gram4-superlative     syntactic   \n",
       "\n",
       "      candidate_1 candidate_2 candidate_3 candidate_4  ... candidate_7  \\\n",
       "0         biggest       major     largest        best  ...       first   \n",
       "1       brightest    brighter     shining   strongest  ...     deepest   \n",
       "2         coolest     hottest    greatest      lowest  ...      finest   \n",
       "3         darkest     deepest    darkened   deadliest  ...      darker   \n",
       "4         easiest    toughest     biggest    greatest  ...     hardest   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "27139     highest     largest     leading     biggest  ...       among   \n",
       "27140     hottest     longest      finest     largest  ...      newest   \n",
       "27141     largest       small       major        most  ...      across   \n",
       "27142     longest     largest     history        most  ...     biggest   \n",
       "27143      lowest     highest     largest        high  ...     biggest   \n",
       "\n",
       "      candidate_8      candidate_9 candidate_10 freq_subset quantile  \\\n",
       "0         leading              one          key       30000     0.25   \n",
       "1         weakest             best      vibrant       30000     0.25   \n",
       "2       strongest         toughest      deepest       30000     0.25   \n",
       "3          lowest          longest     darkness       30000     0.25   \n",
       "4          easier        difficult      longest       30000     0.25   \n",
       "...           ...              ...          ...         ...      ...   \n",
       "27139         low       throughout      history       30000     0.50   \n",
       "27140      heated  fastest_growing     favorite       30000     0.50   \n",
       "27141     leading        important        known       30000     0.50   \n",
       "27142        part           active  established       30000     0.50   \n",
       "27143      active       relatively      smaller       30000     0.50   \n",
       "\n",
       "          rcond                     measure  top@k overall_accuracy  \n",
       "0      0.071726                naive_cosine      1         0.911681  \n",
       "1      0.071726                naive_cosine      1         0.911681  \n",
       "2      0.071726                naive_cosine      1         0.911681  \n",
       "3      0.071726                naive_cosine      1         0.911681  \n",
       "4      0.071726                naive_cosine      1         0.911681  \n",
       "...         ...                         ...    ...              ...  \n",
       "27139  0.086893  mahalanobis_shifted_cosine     10         0.992308  \n",
       "27140  0.086893  mahalanobis_shifted_cosine     10         0.992308  \n",
       "27141  0.086893  mahalanobis_shifted_cosine     10         0.992308  \n",
       "27142  0.086893  mahalanobis_shifted_cosine     10         0.992308  \n",
       "27143  0.086893  mahalanobis_shifted_cosine     10         0.992308  \n",
       "\n",
       "[27144 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innovative-bronze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91168091, 0.9957265 , 1.        , 0.84757835, 0.97720798,\n",
       "       0.99002849, 0.9985755 , 0.83475783, 0.96011396, 0.98575499,\n",
       "       0.91538462, 0.9974359 , 0.82051282, 0.96666667, 0.98974359,\n",
       "       0.8025641 , 0.94102564, 0.97692308, 0.82307692, 0.80512821,\n",
       "       0.94358974, 0.85128205, 0.98205128, 0.99487179, 0.83333333,\n",
       "       0.96153846, 0.98717949, 0.99230769])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radio-sight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall_accuracy\n",
       "1.000000    4524\n",
       "0.997436    3120\n",
       "0.989744    2340\n",
       "0.915385    1560\n",
       "0.995726    1404\n",
       "0.976923    1170\n",
       "0.966667    1170\n",
       "0.823077     780\n",
       "0.941026     780\n",
       "0.802564     780\n",
       "0.847578     702\n",
       "0.977208     702\n",
       "0.990028     702\n",
       "0.998575     702\n",
       "0.834758     702\n",
       "0.960114     702\n",
       "0.985755     702\n",
       "0.911681     702\n",
       "0.994872     390\n",
       "0.987179     390\n",
       "0.961538     390\n",
       "0.833333     390\n",
       "0.805128     390\n",
       "0.982051     390\n",
       "0.851282     390\n",
       "0.943590     390\n",
       "0.820513     390\n",
       "0.992308     390\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seasonal-deployment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-reynolds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
