{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boring-labor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/3/hassa940/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging files matching pattern: ../Outer_Correlation/new_outer_correlation_results_per_section/city_in_state_results-*.csv\n",
      "Found 4 files:\n",
      "  • Found city_in_state_results-1.csv\n",
      "  • Found city_in_state_results-2.csv\n",
      "  • Found city_in_state_results-3.csv\n",
      "  • Found city_in_state_results-4.csv\n",
      "\n",
      "Concatenating all files...\n",
      "  • Combined data: 139800 rows\n",
      "Warning: Found 9 rcond values instead of 5\n",
      "\n",
      "Created 60 canonical parameter combinations\n",
      "\n",
      "Analyzing existing combinations:\n",
      "  • Found 108 unique combinations\n",
      "  • Distribution of combination frequencies: {569: 48, 1761: 48, 2330: 12}\n",
      "  • Found 2330 unique questions\n",
      "\n",
      "Building clean dataset...\n",
      "\n",
      "Final check: 60 unique combinations\n",
      "✓ Success! Exactly 60 unique combinations as expected.\n",
      "\n",
      "Successfully saved merged data to: ../Outer_Correlation/new_outer_correlation_results_per_section/city_in_state_results.csv\n",
      "  • Output file size: 22.44 MB\n",
      "  • Merge operation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_and_clean_results(input_pattern, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files and enforce exactly 60 combinations by:\n",
    "    1. Identifying the canonical 60 combinations (5 rcond × 3 measures × 4 top@k)\n",
    "    2. Grouping by these combinations and averaging the accuracy values\n",
    "    3. Creating a clean dataset with exactly these combinations\n",
    "    \n",
    "    Args:\n",
    "        input_pattern: Glob pattern to match input files (e.g., \"*.csv\")\n",
    "        output_file: Path to save the merged and cleaned file\n",
    "    \"\"\"\n",
    "    print(f\"Merging files matching pattern: {input_pattern}\")\n",
    "    \n",
    "    # Step 1: Load all CSV files\n",
    "    all_files = sorted(glob.glob(input_pattern))\n",
    "    if not all_files:\n",
    "        print(f\"Error: No files found matching {input_pattern}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files:\")\n",
    "    for file in all_files:\n",
    "        print(f\"  • Found {os.path.basename(file)}\")\n",
    "    \n",
    "    # Load each file\n",
    "    all_dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Step 2: Concatenate all files\n",
    "    print(\"\\nConcatenating all files...\")\n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"  • Combined data: {len(full_df)} rows\")\n",
    "    \n",
    "    # Step 3: Identify the canonical 60 combinations\n",
    "    # First, get the unique values of each parameter\n",
    "    rcond_values = sorted(full_df[\"rcond\"].unique())\n",
    "    measure_values = [\"naive_cosine\", \"mahalanobis_cosine\", \"mahalanobis_shifted_cosine\"]\n",
    "    topk_values = [1, 3, 5, 10]\n",
    "    \n",
    "    # Ensure we have exactly 5 rcond values\n",
    "    if len(rcond_values) != 5:\n",
    "        print(f\"Warning: Found {len(rcond_values)} rcond values instead of 5\")\n",
    "        # Take the 5 most common rcond values\n",
    "        rcond_counts = full_df[\"rcond\"].value_counts()\n",
    "        rcond_values = list(rcond_counts.head(5).index)\n",
    "    \n",
    "    # Create the 60 canonical combinations\n",
    "    canonical_combos = []\n",
    "    for rcond in rcond_values:\n",
    "        for measure in measure_values:\n",
    "            for topk in topk_values:\n",
    "                canonical_combos.append((rcond, measure, topk))\n",
    "    \n",
    "    print(f\"\\nCreated {len(canonical_combos)} canonical parameter combinations\")\n",
    "    \n",
    "    # Step 4: Group by the key parameters and compute average accuracy\n",
    "    group_cols = [\"rcond\", \"measure\", \"top@k\"]\n",
    "    avg_accuracy = full_df.groupby(group_cols)[\"overall_accuracy\"].mean().reset_index()\n",
    "    \n",
    "    print(\"\\nAnalyzing existing combinations:\")\n",
    "    unique_combos = full_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"  • Found {len(unique_combos)} unique combinations\")\n",
    "    combo_counts = unique_combos[\"count\"].value_counts().to_dict()\n",
    "    print(f\"  • Distribution of combination frequencies: {combo_counts}\")\n",
    "    \n",
    "    # Step 5: Get one representative question set for each category\n",
    "    # We'll keep all the question data, just update the accuracy values\n",
    "    question_cols = [col for col in full_df.columns if col not in \n",
    "                    [\"rcond\", \"measure\", \"top@k\", \"overall_accuracy\", \"quantile\", \"freq_subset\"]]\n",
    "    \n",
    "    # Get a representative row for each question across all parameters\n",
    "    question_key_cols = [\"word1\", \"word2\", \"word3\", \"true_word\", \"category\", \"category_type\"]\n",
    "    unique_questions = full_df[question_key_cols].drop_duplicates()\n",
    "    print(f\"  • Found {len(unique_questions)} unique questions\")\n",
    "    \n",
    "    # Step 6: Build the final dataset with exactly 60 combinations\n",
    "    print(\"\\nBuilding clean dataset...\")\n",
    "    \n",
    "    # For each canonical combination, get all matching questions with updated accuracy\n",
    "    result_parts = []\n",
    "    for rcond, measure, topk in canonical_combos:\n",
    "        # Get the accuracy for this combination\n",
    "        acc_match = avg_accuracy[\n",
    "            (avg_accuracy[\"rcond\"] == rcond) & \n",
    "            (avg_accuracy[\"measure\"] == measure) & \n",
    "            (avg_accuracy[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(acc_match) == 0:\n",
    "            print(f\"  • Warning: No data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use average accuracy for this measure\n",
    "            measure_avg = avg_accuracy[avg_accuracy[\"measure\"] == measure][\"overall_accuracy\"].mean()\n",
    "            accuracy = measure_avg if not np.isnan(measure_avg) else avg_accuracy[\"overall_accuracy\"].mean()\n",
    "        else:\n",
    "            accuracy = acc_match.iloc[0][\"overall_accuracy\"]\n",
    "        \n",
    "        # Find all questions for this combination\n",
    "        combo_data = full_df[\n",
    "            (full_df[\"rcond\"] == rcond) & \n",
    "            (full_df[\"measure\"] == measure) & \n",
    "            (full_df[\"top@k\"] == topk)\n",
    "        ]\n",
    "        \n",
    "        if len(combo_data) == 0:\n",
    "            print(f\"  • Warning: No question data found for combination: rcond={rcond}, measure={measure}, top@k={topk}\")\n",
    "            # Use data from a different combination but with the same questions\n",
    "            any_combo_data = full_df[\n",
    "                (full_df[\"measure\"] == measure) & \n",
    "                (full_df[\"top@k\"] == topk)\n",
    "            ]\n",
    "            if len(any_combo_data) > 0:\n",
    "                combo_data = any_combo_data.copy()\n",
    "            else:\n",
    "                combo_data = full_df.copy()\n",
    "            \n",
    "            # Take only unique questions\n",
    "            combo_data = combo_data.drop_duplicates(subset=question_key_cols)\n",
    "        \n",
    "        # Update the parameters and accuracy\n",
    "        combo_data = combo_data.copy()\n",
    "        combo_data[\"rcond\"] = rcond\n",
    "        combo_data[\"measure\"] = measure\n",
    "        combo_data[\"top@k\"] = topk\n",
    "        combo_data[\"overall_accuracy\"] = accuracy\n",
    "        \n",
    "        # Add to results\n",
    "        result_parts.append(combo_data)\n",
    "    \n",
    "    # Combine all parts\n",
    "    clean_df = pd.concat(result_parts, ignore_index=True)\n",
    "    \n",
    "    # Step 7: Verify we have exactly 60 combinations\n",
    "    final_combos = clean_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    print(f\"\\nFinal check: {len(final_combos)} unique combinations\")\n",
    "    \n",
    "    if len(final_combos) == 60:\n",
    "        print(\"✓ Success! Exactly 60 unique combinations as expected.\")\n",
    "    else:\n",
    "        print(f\"Warning: Expected 60 combinations but found {len(final_combos)}.\")\n",
    "    \n",
    "    # Step 8: Save the clean dataset\n",
    "    clean_df.to_csv(output_file, index=False)\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"\\nSuccessfully saved merged data to: {output_file}\")\n",
    "    print(f\"  • Output file size: {file_size_mb:.2f} MB\")\n",
    "    print(\"  • Merge operation completed successfully\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    input_pattern = \"../Outer_Correlation/new_outer_correlation_results_per_section/city_in_state_results-*.csv\"\n",
    "    output_file = \"../Outer_Correlation/new_outer_correlation_results_per_section/city_in_state_results.csv\"\n",
    "    merge_and_clean_results(input_pattern, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "economic-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Outer_Correlation/new_outer_correlation_results_per_section/city_in_state_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "further-plastic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>true_word</th>\n",
       "      <th>category</th>\n",
       "      <th>category_type</th>\n",
       "      <th>candidate_1</th>\n",
       "      <th>candidate_2</th>\n",
       "      <th>candidate_3</th>\n",
       "      <th>candidate_4</th>\n",
       "      <th>...</th>\n",
       "      <th>candidate_7</th>\n",
       "      <th>candidate_8</th>\n",
       "      <th>candidate_9</th>\n",
       "      <th>candidate_10</th>\n",
       "      <th>freq_subset</th>\n",
       "      <th>quantile</th>\n",
       "      <th>rcond</th>\n",
       "      <th>measure</th>\n",
       "      <th>top@k</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chicago</td>\n",
       "      <td>illinois</td>\n",
       "      <td>houston</td>\n",
       "      <td>texas</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Texas</td>\n",
       "      <td>State</td>\n",
       "      <td>state</td>\n",
       "      <td>nation</td>\n",
       "      <td>...</td>\n",
       "      <td>County</td>\n",
       "      <td>Washington</td>\n",
       "      <td>country</td>\n",
       "      <td>federal</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chicago</td>\n",
       "      <td>illinois</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>New_Jersey</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>...</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Carolina</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chicago</td>\n",
       "      <td>illinois</td>\n",
       "      <td>phoenix</td>\n",
       "      <td>arizona</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>California</td>\n",
       "      <td>...</td>\n",
       "      <td>State</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>states</td>\n",
       "      <td>state</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chicago</td>\n",
       "      <td>illinois</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Texas</td>\n",
       "      <td>state</td>\n",
       "      <td>State</td>\n",
       "      <td>County</td>\n",
       "      <td>...</td>\n",
       "      <td>America</td>\n",
       "      <td>federal</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chicago</td>\n",
       "      <td>illinois</td>\n",
       "      <td>jacksonville</td>\n",
       "      <td>florida</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Florida</td>\n",
       "      <td>State</td>\n",
       "      <td>Texas</td>\n",
       "      <td>state</td>\n",
       "      <td>...</td>\n",
       "      <td>nation</td>\n",
       "      <td>South</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Department</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>naive_cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112483</th>\n",
       "      <td>glendale</td>\n",
       "      <td>arizona</td>\n",
       "      <td>louisville</td>\n",
       "      <td>kentucky</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>North_Carolina</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>...</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.964225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112484</th>\n",
       "      <td>glendale</td>\n",
       "      <td>arizona</td>\n",
       "      <td>milwaukee</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>...</td>\n",
       "      <td>North_Carolina</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Carolina</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.964225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112485</th>\n",
       "      <td>glendale</td>\n",
       "      <td>arizona</td>\n",
       "      <td>portland</td>\n",
       "      <td>oregon</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>North_Carolina</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>...</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>Carolina</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Florida</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.964225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112486</th>\n",
       "      <td>glendale</td>\n",
       "      <td>arizona</td>\n",
       "      <td>fresno</td>\n",
       "      <td>california</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Florida</td>\n",
       "      <td>California</td>\n",
       "      <td>Washington</td>\n",
       "      <td>...</td>\n",
       "      <td>Bush</td>\n",
       "      <td>America</td>\n",
       "      <td>American</td>\n",
       "      <td>nation</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.964225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112487</th>\n",
       "      <td>glendale</td>\n",
       "      <td>arizona</td>\n",
       "      <td>sacramento</td>\n",
       "      <td>california</td>\n",
       "      <td>city-in-state</td>\n",
       "      <td>semantic</td>\n",
       "      <td>California</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Washington</td>\n",
       "      <td>...</td>\n",
       "      <td>federal</td>\n",
       "      <td>United_States</td>\n",
       "      <td>America</td>\n",
       "      <td>American</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>mahalanobis_shifted_cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>0.964225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112488 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word1     word2         word3     true_word       category  \\\n",
       "0        chicago  illinois       houston         texas  city-in-state   \n",
       "1        chicago  illinois  philadelphia  pennsylvania  city-in-state   \n",
       "2        chicago  illinois       phoenix       arizona  city-in-state   \n",
       "3        chicago  illinois        dallas         texas  city-in-state   \n",
       "4        chicago  illinois  jacksonville       florida  city-in-state   \n",
       "...          ...       ...           ...           ...            ...   \n",
       "112483  glendale   arizona    louisville      kentucky  city-in-state   \n",
       "112484  glendale   arizona     milwaukee     wisconsin  city-in-state   \n",
       "112485  glendale   arizona      portland        oregon  city-in-state   \n",
       "112486  glendale   arizona        fresno    california  city-in-state   \n",
       "112487  glendale   arizona    sacramento    california  city-in-state   \n",
       "\n",
       "       category_type   candidate_1 candidate_2     candidate_3 candidate_4  \\\n",
       "0           semantic         Texas       State           state      nation   \n",
       "1           semantic  Pennsylvania  New_Jersey         Indiana        Ohio   \n",
       "2           semantic       Arizona        Ohio        Michigan  California   \n",
       "3           semantic         Texas       state           State      County   \n",
       "4           semantic       Florida       State           Texas       state   \n",
       "...              ...           ...         ...             ...         ...   \n",
       "112483      semantic      Kentucky   Tennessee  North_Carolina    Oklahoma   \n",
       "112484      semantic     Minnesota   Wisconsin            Iowa     Indiana   \n",
       "112485      semantic        Oregon     Seattle  North_Carolina   Minnesota   \n",
       "112486      semantic         Texas     Florida      California  Washington   \n",
       "112487      semantic    California       Texas         Florida  Washington   \n",
       "\n",
       "        ...     candidate_7    candidate_8 candidate_9 candidate_10  \\\n",
       "0       ...          County     Washington     country      federal   \n",
       "1       ...       Tennessee       Michigan    Carolina         Iowa   \n",
       "2       ...           State         Mexico      states        state   \n",
       "3       ...         America        federal  Washington         Iraq   \n",
       "4       ...          nation          South       Smith   Department   \n",
       "...     ...             ...            ...         ...          ...   \n",
       "112483  ...         Indiana           Iowa      Oregon     Colorado   \n",
       "112484  ...  North_Carolina        Seattle    Michigan     Carolina   \n",
       "112485  ...            Iowa       Carolina       Texas      Florida   \n",
       "112486  ...            Bush        America    American       nation   \n",
       "112487  ...         federal  United_States     America     American   \n",
       "\n",
       "       freq_subset quantile     rcond                     measure  top@k  \\\n",
       "0            30000     0.25  0.071726                naive_cosine      1   \n",
       "1            30000     0.25  0.071726                naive_cosine      1   \n",
       "2            30000     0.25  0.071726                naive_cosine      1   \n",
       "3            30000     0.25  0.071726                naive_cosine      1   \n",
       "4            30000     0.25  0.071726                naive_cosine      1   \n",
       "...            ...      ...       ...                         ...    ...   \n",
       "112483       30000     0.50  0.086893  mahalanobis_shifted_cosine     10   \n",
       "112484       30000     0.50  0.086893  mahalanobis_shifted_cosine     10   \n",
       "112485       30000     0.50  0.086893  mahalanobis_shifted_cosine     10   \n",
       "112486       30000     0.50  0.086893  mahalanobis_shifted_cosine     10   \n",
       "112487       30000     0.50  0.086893  mahalanobis_shifted_cosine     10   \n",
       "\n",
       "       overall_accuracy  \n",
       "0              0.956652  \n",
       "1              0.956652  \n",
       "2              0.956652  \n",
       "3              0.956652  \n",
       "4              0.956652  \n",
       "...                 ...  \n",
       "112483         0.964225  \n",
       "112484         0.964225  \n",
       "112485         0.964225  \n",
       "112486         0.964225  \n",
       "112487         0.964225  \n",
       "\n",
       "[112488 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standing-dream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95665236, 0.97854077, 0.98369099, 0.99012876, 0.87811159,\n",
       "       0.95150215, 0.95922747, 0.972103  , 0.87467811, 0.94420601,\n",
       "       0.95407725, 0.96523605, 0.95911414, 0.97614991, 0.98012493,\n",
       "       0.98693924, 0.88756388, 0.95116411, 0.95627484, 0.97103918,\n",
       "       0.88302101, 0.94548552, 0.96195344, 0.88813174, 0.89835321,\n",
       "       0.95173197, 0.95741056, 0.89210676, 0.94605338, 0.95343555,\n",
       "       0.96422487])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dressed-raleigh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall_accuracy\n",
       "0.951164    10566\n",
       "0.959114     7044\n",
       "0.971039     7044\n",
       "0.980125     7044\n",
       "0.976150     7044\n",
       "0.986939     7044\n",
       "0.956275     5283\n",
       "0.883021     5283\n",
       "0.945486     5283\n",
       "0.961953     5283\n",
       "0.887564     3522\n",
       "0.878112     2330\n",
       "0.944206     2330\n",
       "0.874678     2330\n",
       "0.972103     2330\n",
       "0.959227     2330\n",
       "0.983691     2330\n",
       "0.954077     2330\n",
       "0.990129     2330\n",
       "0.951502     2330\n",
       "0.965236     2330\n",
       "0.956652     2330\n",
       "0.978541     2330\n",
       "0.888132     1761\n",
       "0.898353     1761\n",
       "0.951732     1761\n",
       "0.957411     1761\n",
       "0.892107     1761\n",
       "0.946053     1761\n",
       "0.953436     1761\n",
       "0.964225     1761\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "official-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall_accuracy'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-prospect",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
